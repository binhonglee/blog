<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Opinionated Engineer on BinHong Lee&#39;s Blog</title>
    <link>https://binhong.me/blog/tags/the-opinionated-engineer/</link>
    <description>Recent content in The Opinionated Engineer on BinHong Lee&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>binhong@binhong.me (BinHong Lee)</managingEditor>
    <webMaster>binhong@binhong.me (BinHong Lee)</webMaster>
    <lastBuildDate>Fri, 14 Nov 2025 00:00:00 -0800</lastBuildDate><atom:link href="https://binhong.me/blog/tags/the-opinionated-engineer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Communicating Effectively</title>
      <link>https://binhong.me/blog/2025-09-19-communicating-effectively/</link>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-09-19-communicating-effectively/</guid>
      <description>&lt;p&gt;It’s a common problem at work where you see a problem but you’re having a hard time trying to convince people to support your attempt at fixing it. Worse still, they might not even see it as a problem (or they don’t think it’s something that warrants any attention from anyone). I’m approaching this piece mostly from an &lt;em&gt;“I need my leadership / partners to understand what I do is important”&lt;/em&gt; perspective. But the ideology can similarly be used on other occasions like selling an idea to your peers etc. Ideally you start thinking about this as you try to get the green light to pursue your project instead of attempting to justify it &lt;em&gt;after&lt;/em&gt; you’ve already invested your time into it with little to show for it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#understand-your-audience&#34;&gt;
    &lt;h2 id=&#34;understand-your-audience&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Understand your audience&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Whoever you’re talking to, they have their own perspective and priorities. They care about certain things. Make sure to understand that and position your proposal from &lt;em&gt;their&lt;/em&gt; perspective. One common communication gap is where someone keeps trying to tell their leaders &lt;em&gt;“how hard they / the team has been working”&lt;/em&gt; - which is cool and all - but your leaders likely want to know more about &lt;em&gt;“what’s in it for me?”&lt;/em&gt;. For example, instead of telling your manager that “I put in 60 hours a week all of last month to make this happen,” it should be more like “I shipped x MAU which is x% of the team / org goal” or “I worked overtime to handle multiple unexpected surprises (have documentation for this) to ensure that we can ship / launch / test {org priority project} in a timely manner.” Changing the framing of your statement (by empathizing with your audience) makes it significantly more appealing to your target audience.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#strategic-positioning&#34;&gt;
    &lt;h2 id=&#34;strategic-positioning&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Strategic positioning&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When you pitch a project proposal, you need strong reasoning on why you think it will be worth the time investment. In most cases, you’d be making the case using some sort of metrics or goal that the project is expected to improve. However, that’s not always the case. If you have a goal metric to chase but this project doesn’t directly affect that, think of how it’s affected indirectly. In my previous piece on dev tool valuation, I mentioned &lt;a href=&#34;https://binhong.me/blog/2025-07-18-understanding-value-of-dev-tools/#cost-of-engineers&#34;&gt;engineering cost&lt;/a&gt; being the most straightforward measurement available.&lt;/p&gt;
&lt;p&gt;On the other hand, if there’s a &lt;em&gt;larger org-priority project&lt;/em&gt;, think of how to align your project with it. You might need to make some small changes in the project pitch, but the idea is that you don’t have to build your case from scratch. Instead, you are just piggybacking on an established case by explaining how your project would &lt;em&gt;help&lt;/em&gt; with the goal of completing the larger project. This sort of positioning not only helps reduce the amount of foundational work needed to make people understand the &lt;em&gt;why&lt;/em&gt; but also ensures that your project will continue to receive attention as a side effect of now being associated with the larger project.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#reprioritization&#34;&gt;
    &lt;h2 id=&#34;reprioritization&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Reprioritization&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Prioritization changes all the time. When there’s a top-down change in priority, make sure to ask clarifying questions to understand the reasoning behind the shift. From there, you have to then recalibrate your (team’s) projects to make sure they’re still aligned with the higher-level goals. On some occasions, this could also mean that you would have to recommend priority shifts on ongoing / planned projects depending on the leadership goal change. For example, you&amp;rsquo;re in the core infrastructure team and the company decides to pivot to &amp;ldquo;everything AI&amp;rdquo;. Make sure to understand &amp;ldquo;how AI&amp;rdquo; and potentially shift your prioritization around (like adding better GPU support, direct Ollama support etc.) to fit the leadership goal change.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#goal-setting&#34;&gt;
    &lt;h2 id=&#34;goal-setting&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Goal setting&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Management (and shareholders) love &lt;em&gt;goals&lt;/em&gt;. They paint a picture of predictability and allow for an easy way to evaluate how you (or the team) are doing. The higher level you get (with bigger scope and projects), the goal would be closer to company-level goals. The hierarchy probably goes something like this: (company-level goal) make more money -&amp;gt; sell more ads -&amp;gt; get more users -&amp;gt; keep existing users returning (team-level goal) etc. You don’t have to set the goal on how to make the company more money, but you can set a goal on how to keep existing users returning which, by proxy, would have your impact cascaded up.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Admittedly, this is something even I sometimes forget or overlook when pitching a project. The key insight is that effective communication isn’t about having the best technical solution but rather, it&amp;rsquo;s about meeting people where they are and speaking their language. When you frame your ideas through your audience’s priorities and align with existing organizational momentum, you’re offering value in return of their support.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hidden Privacy / Security Pitfalls</title>
      <link>https://binhong.me/blog/2025-08-23-hidden-privacy-and-security-pitfalls/</link>
      <pubDate>Sat, 23 Aug 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-08-23-hidden-privacy-and-security-pitfalls/</guid>
      <description>&lt;p&gt;Most software engineers think of this as &amp;ldquo;someone else&amp;rsquo;s problem&amp;rdquo; (usually a privacy engineer or security engineer), but realistically, there&amp;rsquo;s only so much stuff (bad code lol) they can catch. Especially if you work in some of the more sensitive areas, it&amp;rsquo;s not realistic to have them do code audits for every change. This means that you as the code author (or reviewer) need to at least pick up some basic instincts about things that can pose risks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: I&amp;rsquo;m neither a privacy engineer nor a security engineer (but a software engineer) so take this mostly as a way to raise awareness about considering some of these pitfalls while building instead of as an expert opinion on what to do.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#timing-attack&#34;&gt;
    &lt;h2 id=&#34;timing-attack&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Timing attack&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Even if you return the same response every time, you might still be leaking data (or mostly inference) based on the timing it took the server to respond. As an example, a typical email + password login system probably looks like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Hash password&lt;/li&gt;
&lt;li&gt;Email lookup&lt;/li&gt;
&lt;li&gt;Compare hash&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, considering that password hashing can take quite a bit of time and resources, we could rearrange the order to make it more efficient by making use of &amp;ldquo;early returns.&amp;rdquo; Something like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Email lookup&lt;/li&gt;
&lt;li&gt;Hash password&lt;/li&gt;
&lt;li&gt;Compare hash&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this new setup, you&amp;rsquo;d only do password hashing when an email already exists in your record, thus saving time and processing power from unnecessary password hashing. However, you&amp;rsquo;re also returning your network request much sooner. This becomes an unintentional leak where a bad actor can use it to tell if an email exists in your database or not by doing some timed response comparison. My favorite solution to this problem (when applicable) is to add a random response delay—not long enough to significantly affect user experience, but just long enough to throw an attacker off.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#scraping&#34;&gt;
    &lt;h2 id=&#34;scraping&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Scraping&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Generally the &lt;em&gt;easy&lt;/em&gt; solution here is to slap a rate limit on it and call it a day. The problem, however, is that you need to realize not all endpoints are created equal. Most evidently, you have an endpoint that returns info about a user when given an ID, while you also have an endpoint that returns info about &lt;strong&gt;a list of users&lt;/strong&gt; when given a list of IDs. Both of these endpoints can&amp;rsquo;t just share the same rate limiting because scraping the latter endpoint is &lt;code&gt;list_length&lt;/code&gt;x more &lt;em&gt;efficient&lt;/em&gt; than the former.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#credential-stuffing&#34;&gt;
    &lt;h2 id=&#34;credential-stuffing&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Credential stuffing&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is usually the result of password reuse. While you can &lt;em&gt;technically&lt;/em&gt; do credential stuffing with just randomly enumerated credentials, the cost is unlikely to be worth the outcome for the attacker (unless there are some specific exceptions). Since people reuse passwords and there have been many previous hacks (unfortunately containing plaintext passwords), your perfectly secure authentication system becomes vulnerable not because of your code, but because of breaches elsewhere. That said, these attacks are &lt;em&gt;usually&lt;/em&gt; more obvious, coming from an &lt;em&gt;expected&lt;/em&gt; set of IPs (all from the same location or same VPS provider, etc.).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#contactpoint-disclosure&#34;&gt;
    &lt;h2 id=&#34;contactpoint-disclosure&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Contactpoint disclosure&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;A little different from the rest of the list, this is more of a &lt;em&gt;type&lt;/em&gt; of disclosure than a &lt;em&gt;method&lt;/em&gt; of disclosure. While many systems are more conservative and opt for the route of never even disclosing if a specific contactpoint (email / phone number) is even associated with the platform, some do allow for it. However, this will require a bit more care in handling such disclosure where we don&amp;rsquo;t want to disclose &lt;strong&gt;other&lt;/strong&gt; contactpoints being connected to a given contactpoint (even through inference). The main reason being that this risks leaking identities behind pseudonyms where people can &lt;em&gt;connect the dots&lt;/em&gt;.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#rate-limiting&#34;&gt;
    &lt;h2 id=&#34;rate-limiting&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Rate limiting&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Rate limiting is both a valuable tool and also a potential risk of exposure. Most evidently, rate limiting helps reduce (and hopefully prevent) scraping and credential stuffing attacks. However, in some systems, rate limits are set on a per-account basis, making it a risk of exposure as you can do targeted attacks with some reasonable correlation to get information out of it.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Condition: email A and phone A are both owned by the same account while email B is not.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;try (and fail) login with email A -&amp;gt; no rate limit&lt;/li&gt;
&lt;li&gt;try (and fail) login with email B -&amp;gt; no rate limit&lt;/li&gt;
&lt;li&gt;try (and fail) login with phone A -&amp;gt; no rate limit&lt;/li&gt;
&lt;li&gt;keep trying email A until it&amp;rsquo;s rate limited&lt;/li&gt;
&lt;li&gt;try (and fail) login with email B -&amp;gt; no rate limit (since different account)&lt;/li&gt;
&lt;li&gt;try (and fail) login with phone A -&amp;gt; &lt;strong&gt;rate limited&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are many reasons (or other situations) where it&amp;rsquo;s not immediately obvious how introducing an account-based rate limit can be a risk.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#phone-recycling&#34;&gt;
    &lt;h2 id=&#34;phone-recycling&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Phone recycling&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Many platforms rely on phone number verification to authenticate your ownership of the account, but phone number ownership isn&amp;rsquo;t exactly &lt;em&gt;immutable&lt;/em&gt;. In fact, phone numbers commonly switch hands in most countries for a variety of reasons (especially as people move across countries). Unfortunately, this is a much trickier problem to tackle due to the wide variety of carriers spread across different countries having different processes and regulations around it.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#email-recycling&#34;&gt;
    &lt;h2 id=&#34;email-recycling&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Email recycling&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;While email recycling isn&amp;rsquo;t as common as phone recycling, it still persists (especially those with custom domains). In fact, many years ago one of the largest email service providers actually allowed anyone to register expired email addresses. This became a major attack vector on Facebook since many people back then set their emails to be shown publicly on their Facebook profiles, while also simultaneously stopping using those same email addresses - leaving them to expire. Thankfully, most email providers have learned of such attack vectors and no longer allow deactivated / deleted email addresses to be re-registered &lt;em&gt;ever&lt;/em&gt;.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;These pitfalls often emerge from well-intentioned optimizations that look perfectly reasonable in isolation. The key is developing a mental checklist of common attack patterns rather than becoming paranoid about every line of code. Security isn&amp;rsquo;t just about preventing obvious attacks but instead, it&amp;rsquo;s to think through the subtle ways your system behavior can be observed and / or exploited.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Firefighting Heroes</title>
      <link>https://binhong.me/blog/2025-08-01-firefighting-heroes/</link>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-08-01-firefighting-heroes/</guid>
      <description>&lt;p&gt;We love celebrating heroes in general. So when a hero shows up to save the day, it&amp;rsquo;s only natural that we make sure to recognize their contribution. While it might feel counterintuitive, we should strive to not need heroes entirely instead of hoping that next time when tragedy strikes, a hero will turn up once again. This is not to diminish the value of heroes. They still play a very important role in firefighting and are doing the right thing in an urgent situation. However, &lt;em&gt;in a perfect world&lt;/em&gt;, they shouldn&amp;rsquo;t be necessary at all and we should at least aspire to work towards building a system with that level of reliability.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#bian-que-and-his-brothers&#34;&gt;
    &lt;h2 id=&#34;bian-que-and-his-brothers&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Bian Que and his brothers&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;If you&amp;rsquo;ve never heard of the story about &lt;em&gt;Bian Que and his brothers&lt;/em&gt;, here&amp;rsquo;s a &lt;a href=&#34;https://www.meta.ai/@binhonglee/prompt/iS2XuvsxNDq&#34;&gt;Meta AI summary of it&lt;/a&gt; with all the relevant details:&lt;/p&gt;
&lt;blockquote class=&#34;long_quote&#34;&gt;
    &lt;p&gt;Bian Que was a renowned Chinese physician who lived during the Warring States period (475-221 BCE). According to legend, Bian Que and his two older brothers were all skilled in medicine.&lt;/p&gt;
&lt;p&gt;The story goes that the eldest brother treated illnesses before they became apparent, the second brother treated illnesses when they were still subtle, and Bian Que treated illnesses when they were severe and obvious.&lt;/p&gt;
&lt;p&gt;When people saw Bian Que&amp;rsquo;s dramatic cures, they credited him with great skill. However, Bian Que said that his brothers were actually more skilled because they treated problems before they became serious.&lt;/p&gt;
&lt;p&gt;This story highlights the importance of preventive medicine and early intervention in Chinese medical philosophy.&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;I mention this because one time when I was complaining about all the fires happening in our org and how I keep getting looped into them to help put them out, my then tech lead cited this story to me (which is a little offensive 😡 but she&amp;rsquo;s not wrong lol). I think in general, this is an accurate depiction of the &lt;strong&gt;misplaced prioritization and recognition&lt;/strong&gt; where we consistently reward &amp;ldquo;heroes&amp;rdquo; solving issues rather than those who worked to prevent fires from happening to begin with simply due to visibility and measurability.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#automate-whenever-possible&#34;&gt;
    &lt;h2 id=&#34;automate-whenever-possible&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Automate whenever possible&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I remember once reading a Site Reliability Engineer write (verbatim), &lt;em&gt;&amp;ldquo;my job is to automate myself out of my own job&amp;rdquo;&lt;/em&gt; which I thought was an interesting way to look at things but also an obvious starting point. To do away with the reliance on firefighting heroes, automate the things they do whenever possible. As an example, when something breaks and you find an automated test for that specific breakage, have a system that automatically runs bisect against said test to find the blame commit causing the breakage. Taking it a step further, the system can run all these automated tests every time before release and if anything is broken, bisect to find the blame commit and revert it, then run everything again until there&amp;rsquo;s a good stable version that&amp;rsquo;s ready for release.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#prevention--cure&#34;&gt;
    &lt;h2 id=&#34;prevention--cure&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Prevention &amp;gt; Cure&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;As the idiom says, &lt;em&gt;&amp;ldquo;prevention is better than cure&amp;rdquo;&lt;/em&gt;. In a previous piece about &lt;a href=&#34;https://binhong.me/blog/2025-05-30-no-blame-sev-culture/#process-over-people&#34;&gt;Process over People&lt;/a&gt;, I&amp;rsquo;ve talked about the focus on building process as a preventative measure. This is a similar point where we should ideally focus more on &lt;strong&gt;preventing&lt;/strong&gt; outages from even happening instead of just putting them out &lt;strong&gt;after&lt;/strong&gt; they&amp;rsquo;re already on fire. The hard part however is - yet again - that the visibility of good prevention work is never as prominent as good firefighting work. There is a two-fold solution to this (where ideally both need to happen). Firstly, if you (or you know someone) are doing important prevention work, ensure people are aware of their work. Secondly, leadership needs to prominently acknowledge the value of such work and reflect it as such in &lt;em&gt;whichever-way-they-usually-do-recognition&lt;/em&gt; and in performance reviews.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#accountability-on-reliability&#34;&gt;
    &lt;h2 id=&#34;accountability-on-reliability&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Accountability on reliability&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;One of the common pitfalls here is that, once something is shipped, the responsibility to keep it running falls on the team (whoever is on-call at the time of incident) instead of the individual who shipped an &lt;em&gt;incomplete&lt;/em&gt; product. This is something I&amp;rsquo;ve previously touched on &lt;a href=&#34;https://binhong.me/blog/2025-05-30-no-blame-sev-culture/#no-blame--no-responsibility&#34;&gt;here&lt;/a&gt;. If someone is taking credit for the successful launch (and all its underlying &lt;strong&gt;impact&lt;/strong&gt;), they should also be held accountable if the launch causes reliability issues (be it during or shortly after the launch). As to how &lt;em&gt;accountability&lt;/em&gt; looks, it can range anywhere from taking ownership of resolving the issue long term, to being penalized in their performance review cycle for an incomplete launch. There&amp;rsquo;s quite a bit of nuance here depending on factors like the scale of outages, any visible warning signs, how preventable the issue was, etc.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is as much (if not more) of a culture problem as it is a technical problem. Building reliable systems isn&amp;rsquo;t just about writing better code or implementing more monitoring. It requires fundamentally shifting how we think about and reward engineering work. We need to move away from the hero worship that celebrates last-minute saves and instead build cultures that value the unglamorous work of prevention, automation, and long-term system health. Heroes will always be needed in truly exceptional circumstances, but if your organization consistently relies on them to keep the lights on, it&amp;rsquo;s a sign that your systems, processes, and culture need serious attention. The goal isn&amp;rsquo;t to eliminate heroes entirely but to build systems so robust that heroic interventions become rare exceptions rather than regular occurrences.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Major Incident Runbook</title>
      <link>https://binhong.me/blog/2025-07-25-major-incident-runbook/</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-07-25-major-incident-runbook/</guid>
      <description>&lt;p&gt;I wrote a similar version of this internally at Meta a few years ago for my org after finding myself in the middle of a few SEV1s in a row &amp;ndash; and being consulted / asked for support in other similar situations. I thought this might be something useful to share (as a public version) as well. This won&amp;rsquo;t be perfectly fitting for all use cases, but having a runbook works as an anchor in the midst of chaos, helping to get you unstuck from &amp;ldquo;what&amp;rsquo;s next?&amp;rdquo;. Admittedly, this is an incomplete runbook that serves more as a template for your team or company to complete with more specific tooling guides (using &lt;em&gt;which&lt;/em&gt; tool to achieve &lt;em&gt;what&lt;/em&gt;, etc.).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#stop-the-bleed&#34;&gt;
    &lt;h2 id=&#34;stop-the-bleed&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Stop the Bleed!&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The highest priority is to stop the bleeding immediately. &lt;em&gt;Flip a killswitch, roll back changes, apply server-side fixes, apply client-side fixes&lt;/em&gt; - &lt;strong&gt;in that order&lt;/strong&gt;. A killswitch generally propagates faster, thus is preferred over code changes, but even then, prioritize rolling back changes instead of forward fixing. Forward fixing adds more unknown factors into the mix (because that&amp;rsquo;s more new code which could now cause new / different problems); rolling back, on the other hand, is more predictable. Server-side fixes over client-side fixes should be pretty obvious since you can guarantee the server version (as the service provider), but you can&amp;rsquo;t always force a client to update (native apps), or there could be some caching involved (web).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#multiple-workstreams&#34;&gt;
    &lt;h2 id=&#34;multiple-workstreams&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Multiple Workstreams&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;During a major incident, there are generally multiple things that can or need to happen in parallel. Break them down into clear workstreams and delegate a domain expert to run each of them. If more things are discovered down the line (or the situation changes), switch up the workstream and get different people (domain experts) involved to run different things. Since you&amp;rsquo;re dealing with an incident (which are usually time sensitive), &lt;strong&gt;never hesitate to call people&lt;/strong&gt;. This needs to be said a lot because people constantly hesitate about false positives or getting on others&amp;rsquo; bad side for inaccurately calling them up. But you won&amp;rsquo;t know what you don&amp;rsquo;t know without getting the domain expert to show up and verify what you&amp;rsquo;re seeing.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#chat-management&#34;&gt;
    &lt;h2 id=&#34;chat-management&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Chat Management&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Alongside having separate workstreams, you should also set up separate chat threads for each workstream to keep the &lt;em&gt;main chat&lt;/em&gt; low on noise. That said, make sure to announce the establishment and / or major milestones of each new workstream clearly within the main chat so everyone relevant is correctly included. You might be in a lot of different chats and things will be chaotic, so you will need to take extra care in ensuring that the right people are in the right chat to allow them to get their different tasks going. Everyone should still be in the main chat, but most discussion should happen in the workstream chat.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#sync-meetings&#34;&gt;
    &lt;h2 id=&#34;sync-meetings&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Sync Meetings&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;There are generally 2 types of sync meetings. First is the &amp;ldquo;everything just happened so we started a meeting to info dump and get everyone up to speed.&amp;rdquo; This meeting will always be chaotic with a lot of people joining to figure out what&amp;rsquo;s happening and what to do next. (There might also be people who only joined out of curiosity, but as long as they aren&amp;rsquo;t interrupting, they are the least of your problems - unless Zoom is not scaling lol.) Set out a clear goal, a set of tasks, and an owner for each of those tasks. Finally, set a follow-up date and time (usually in a few hours) for everyone to regroup with new findings and progress to determine next steps.&lt;/p&gt;
&lt;p&gt;The second type is the follow-up and / or recurring meeting. This will feel more like your regular team standup meeting except with a lot more urgency. It&amp;rsquo;s important to note that if there&amp;rsquo;s any major breakthrough, people shouldn&amp;rsquo;t wait until the next planned meeting time to report (because, well, this is an outage lol) but should instead share it with everyone immediately. This goes back to the previous point about chat management where you (as the incident manager) will need to monitor each workstream chat to catch something like this. After resolving the core issue, there might still be &lt;em&gt;important&lt;/em&gt; cleanup work needed to be handled with a similar level of urgency to prevent the same outage from happening again in a very short time. In these long-running incident cleanups, you might opt to have daily (or even twice-a-day) sync meetings until the cleanup is complete.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;As a whole, you will largely be more like a PM / TPM instead of a regular engineer when handling a lot of this &lt;em&gt;alignment&lt;/em&gt; work. If you think someone else is better suited to handle this (another more senior engineer or your manager, etc.), ask for help while you focus on the thing that you do best (likely as the subject matter expert on investigation or remediation). Remember that incident management is a skill that improves with practice - don&amp;rsquo;t expect to be perfect on your first few incidents, even experienced engineers can feel overwhelmed when everything is on fire. Don&amp;rsquo;t forget to acknowledge the team&amp;rsquo;s efforts since major incidents are stressful for everyone involved, and use this runbook as a starting point while adapting it based on your team&amp;rsquo;s specific needs and continuously improving your processes with each incident.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the Value of Dev Tools</title>
      <link>https://binhong.me/blog/2025-07-18-understanding-value-of-dev-tools/</link>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-07-18-understanding-value-of-dev-tools/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m largely approaching this from the perspective of building an internal dev tool (since this is mostly where my personal experiences are coming from), especially if you&amp;rsquo;re someone who wants to build an internal tool but is having trouble framing it in a way where its value can be understood by the decision makers. I&amp;rsquo;m also excluding situations where you &amp;ldquo;have to&amp;rdquo; build certain tools to track certain information in order to comply with a legal requirement, since I don&amp;rsquo;t think that would be controversial to comply with. I&amp;rsquo;d imagine the framework for using an external dev tool, while similar, also poses extra complexity on paying vs building.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#cost-of-engineers&#34;&gt;
    &lt;h2 id=&#34;cost-of-engineers&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Cost of engineers&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Generally, the simplest way would be to calculate how much time is saved for an employee, multiplied by the cost of hiring the employee. This directly shows how much money the organization is saving by just having this tool around. Pretty straightforward, not a lot of controversy.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#value-generated&#34;&gt;
    &lt;h2 id=&#34;value-generated&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Value generated&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In organizations where engineering is considered a profit center, and not a cost center, it might make more sense to calculate it with the value generated by the employee against the time saved. That said, this is usually much harder to measure compared to the above, thus more uncommon unless the tooling itself is a direct value generator.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#urgency-disaster-recovery&#34;&gt;
    &lt;h2 id=&#34;urgency-disaster-recovery&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Urgency (Disaster Recovery)&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In a disaster recovery situation, the time is significantly more valuable than the average &amp;ldquo;engineering hours&amp;rdquo; since it&amp;rsquo;s directly affecting the bottom line of company profit or other top-line metrics (or even consumer trust of the product). Admittedly I&amp;rsquo;ve never figured out the perfect framing around this, but I&amp;rsquo;d reckon phrasing it as &amp;ldquo;used to resolve x, y, z incidents&amp;rdquo; (and if available, attach the severity level for each of the incidents to it) would suffice in clearly articulating this.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#employee-satisfaction&#34;&gt;
    &lt;h2 id=&#34;employee-satisfaction&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Employee satisfaction&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is one of the more subjective measurements. While you can attempt to quantify it through things like survey responses, it&amp;rsquo;s important to keep in mind that the improvement will automatically become the new baseline. For example, an 8/10 last year will become a 5/10 this year, so you need to improve it &lt;em&gt;even more&lt;/em&gt; to get that same 8/10 again this year. On some occasions, additional improvement will hit the point of diminishing returns and have very little investment value. It&amp;rsquo;s important to take notice and begin pivoting effort into better sustainability.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#one-off-use-case&#34;&gt;
    &lt;h2 id=&#34;one-off-use-case&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;One-off use case&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Depending on the level of effort, one-off use cases can be a bit of a dangerous misdirection / mismanagement of valuable resources (time). It&amp;rsquo;s sometimes hard to notice, but if it takes you longer to build the tool than to just do the work without it, or the tool provides very minimal value compared to the amount of work needed to build (and maintain) the tool, building them makes no sense. Generally, this is usually a rare problem and only exists for top-down asks because leadership might not understand the day-to-day workflow for impacted employees. Instead, we generally see the opposite where even the most valuable / impactful tools take a lot of work to convince leadership that they&amp;rsquo;re worth the investment.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#not-invented-here-syndrome&#34;&gt;
    &lt;h2 id=&#34;not-invented-here-syndrome&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Not-Invented-Here Syndrome&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Another common trap in building internal tools is the Not-Invented-Here Syndrome. This is especially true for mission-critical tools (alert paging, traffic monitoring, authentication etc.) where it would make more sense (for most companies / orgs) both financially and expertise-wise to &lt;em&gt;outsource&lt;/em&gt; them to dedicated product experts in the market, making &amp;ldquo;buying&amp;rdquo; a better option than &amp;ldquo;building&amp;rdquo;. &amp;ldquo;Buying&amp;rdquo; here doesn&amp;rsquo;t necessarily mean monetary purchase either. For example, if your team begins scaling your code base and needs a large-scale cross-stack build tool, it would make more sense to adopt something like &lt;a href=&#34;https://bazel.build/&#34;&gt;Bazel&lt;/a&gt; or &lt;a href=&#34;https://buck.build/&#34;&gt;Buck&lt;/a&gt; instead of building your own new build tool from scratch.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Understanding the value of dev tools requires matching your measurement approach to what resonates with your organization&amp;rsquo;s priorities. The goal isn&amp;rsquo;t just to justify building tools, but to build the right ones which can sometimes be as simple as buying an existing solution or accepting a manual process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Internal Tooling Ideas</title>
      <link>https://binhong.me/blog/2025-07-11-internal-tooling-ideas/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-07-11-internal-tooling-ideas/</guid>
      <description>&lt;p&gt;For years, I built and maintained the only logged-out accessible dev tool set / platform at Meta. That earned me some reputation (in a certain circle) of being &amp;ldquo;the idea guy on internal tools&amp;rdquo;. Whenever I&amp;rsquo;m asked about how I keep coming up with good ideas for valuable tools to build, my go-to answer has been &amp;ldquo;I build tools when I get annoyed while doing my job&amp;rdquo;. I&amp;rsquo;ll try to expand that into more details below.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#repetitive-typing&#34;&gt;
    &lt;h2 id=&#34;repetitive-typing&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Repetitive Typing&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The biggest value of having logged-out accessible dev tools is that you no longer have to log in to access dev tools. Login, while generally not too big of a friction for one-off situations, can be very bothersome if you have to type your credentials over and over again (for dev purposes) as you login (update config) → logout (test) → login (update config) → logout (test). If bypassing or removing the typing aspect is not possible, consider if it can be replaced with multiple choice button selection or just a single copy-paste friendly input.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#unnecessary-waiting--processing&#34;&gt;
    &lt;h2 id=&#34;unnecessary-waiting--processing&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Unnecessary Waiting / Processing&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;There are generally 2 types of waiting problems. The first one is where the wait takes a really long time and can be done asynchronously. Either move the wait into an async manner and integrate a messaging system to notify the author when it&amp;rsquo;s completed, or pre-process them based on known patterns. For example, companies with large native app (iOS / Android) code bases have long build times. You can integrate build tools like &lt;a href=&#34;https://bazel.build/remote/caching&#34;&gt;Bazel with remote caching&lt;/a&gt; so clients don&amp;rsquo;t have to rebuild everything from scratch. Since an engineer will likely want to pull from remote &lt;code&gt;HEAD&lt;/code&gt; (or &lt;code&gt;stable&lt;/code&gt;) daily, you can have a cron job that pulls and builds daily in the morning before the engineer starts working, thus turning their wait block time into an &lt;em&gt;invisible&lt;/em&gt; operation done automagically while they are having their breakfast or morning coffee. The second type is where the wait is one part of the process and can&amp;rsquo;t be (deterministically) pre-processed ahead of time / asynchronously. In this case, consider if bypassing is an option (e.g., skips in &lt;code&gt;dev&lt;/code&gt; but runs in &lt;code&gt;prod&lt;/code&gt;). If not, you can also try making it non-blocking (move it off the main thread, batch processing, backfill etc.) as the user proceeds through the process. If all else fails, you can always turn that part of the process into an async step with an integration to a messaging system notifying the author once it&amp;rsquo;s ready to proceed to the next step.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#device-config--manual-hard-code&#34;&gt;
    &lt;h2 id=&#34;device-config--manual-hard-code&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Device Config &amp;gt; Manual Hard-code&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In many of these cases, you can probably do manual hard-coding to get the intended behavior. However, doing so increases the risk of accidentally committing such hard-coded changes into prod (if not caught during code review). On top of that, when working on a complex code base, the engineer might not be familiar with the code pointer for where the hard-code needs to go. For instance, if you want to bypass the rate-limit of your feature but the rate-limit logic is owned by a separate team in an unfamiliar code base. As an added bonus, this also allows non-technical members of your team (designers, PMs etc.) to do product audits, dogfooding, and in-depth testing independently without needing dedicated engineering support.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#non-deterministic-behaviors-ml-override&#34;&gt;
    &lt;h2 id=&#34;non-deterministic-behaviors-ml-override&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Non-deterministic Behaviors (ML override)&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is somewhat similar to the manual hardcode situation mentioned above where you might want to override an ML decision to manually test out different decision combinations by the model. ML models (and AI) are relatively non-deterministic by design, but when building / testing a product, you want to make sure that you covered all the different possible scenarios. Having an override to switch between different potential responses allows for better coverage on both manual and automated testing, ensuring that your change behaves as intended.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#chore&#34;&gt;
    &lt;h2 id=&#34;chore&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Chore&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is more about employee satisfaction when working on a repetitive task may be boring or frustrating. Having tools to automate such work allows them to explore more interesting / challenging work, thus improving employee satisfaction, even if it doesn&amp;rsquo;t necessarily save a significant amount of time. The best &amp;ldquo;realistic&amp;rdquo; way to measure this would be a rating system where you count the number of employees using it and ask for their feedback (both positive and negative), then compile to show the value it provides.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Building internal tools is about removing friction from your team&amp;rsquo;s daily workflow. The best tools emerge from genuine pain points you experience while doing your job. Start small and focus on the annoyances that happen most frequently. The key is to stay observant of your own frustrations and act on them. Every great internal tool started with someone saying &amp;ldquo;there has to be a better way to do this&amp;rdquo;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Early takes on vibe-coding</title>
      <link>https://binhong.me/blog/2025-07-03-early-takes-on-vibe-coding/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-07-03-early-takes-on-vibe-coding/</guid>
      <description>&lt;p&gt;I keep hearing about vibe-coding and I&amp;rsquo;ve always written the majority of code myself. While at Meta, I got a chance to try out CodeCompose. It worked really well as an autocomplete but when it tried to do anything more than 5 lines at a time, it would - on many occasions - commit bugs that aren&amp;rsquo;t immediately obvious at first sight. Generally, I&amp;rsquo;ve caught them by looking at the generated code and wondering &amp;ldquo;huh this isn&amp;rsquo;t how I&amp;rsquo;d do this, why?&amp;rdquo;. That said, it definitely helped me code and ship faster especially on mundane tasks. Vibe-coding though, seems like taking it to a whole new level (using even less supervision and care on the code being committed).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#perfect-for-small-isolated-problems&#34;&gt;
    &lt;h2 id=&#34;perfect-for-small-isolated-problems&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Perfect for small, isolated problems&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I started my attempt by making Claude code out a GitHub Action workflow file. I have a submodule setup (where a repo is shared and imported across multiple other repos) and wanted to have an automated way to tell how its changes will affect code on other repos while also creating PRs to keep them updated. Seems like a perfectly fine isolated problem to try this out on. I did run out of tokens a few times (being on a free plan) so I had to get creative but it largely worked. I&amp;rsquo;d say it behaved like a normal engineer writing a first version (which isn&amp;rsquo;t perfect) but can understand and work its way through debugging and resolving the issue slowly when given clear information on what went wrong.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#not-for-complex-changes-in-an-intern-size-project&#34;&gt;
    &lt;h2 id=&#34;not-for-complex-changes-in-an-intern-size-project&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Not for complex changes in an &lt;em&gt;intern-size&lt;/em&gt; project&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Note: Using the phrase &amp;ldquo;intern-size&amp;rdquo; here because back then, there was a weird rumor that interns were expected to ship 10k LoC as part of their internship to get return offers in FAANG lol. I don&amp;rsquo;t think it was ever true but definitely a standard people worked towards.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now that I&amp;rsquo;ve got it working on an isolated problem, I wanted to see how it might handle a complex change in a pre-existing project. I have an Android app codebase (for &lt;a href=&#34;https://globetrotte.com&#34;&gt;GlobeTrotte&lt;/a&gt;) with around 8k+ LoC so I decided to try it on there (using SWE-1 from Windsurf). This is the instruction I provided (admittedly a complex one):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;add new navhost to edittripactivity and make each of edit day and edit place a separate screen instead (so it push-and-pop for each small edit)&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;PS: &lt;code&gt;edittripactivity&lt;/code&gt; is a file name (technically &lt;code&gt;EditTripAcitivity.kt&lt;/code&gt; but I think the LLM understood it), &lt;code&gt;navhost&lt;/code&gt; is a concept of &lt;a href=&#34;https://developer.android.com/develop/ui/compose/navigation#create-navhost&#34;&gt;how screen navigation works in Jetpack Compose&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The LLM took 20+ minutes before running out of time which required me to make a &lt;code&gt;continue&lt;/code&gt; call not just once but twice before telling me it was done. &lt;em&gt;It&amp;rsquo;s all chaos from here on out.&lt;/em&gt; It tells me that there are a bunch of errors so it tries to write more code (?) leading to more errors, so more code, then more errors etc. At some point, I mentioned that there were 88 errors and it figured to try compiling and reading the compiler error (instead of looking for them itself) but that barely cut down the number of errors. &lt;strong&gt;I just kept telling it that there were more errors and it just kept trying to code itself out of the mess by adding more code and thus more errors.&lt;/strong&gt; I eventually gave up and ran &lt;code&gt;git checkout .&lt;/code&gt; to clean everything up.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#losing-track-of-signatures&#34;&gt;
    &lt;h2 id=&#34;losing-track-of-signatures&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Losing track of signatures&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;At some point, it started making up stuff that either existed with a different name, or something that it thought should exist but didn&amp;rsquo;t (or it forgot to add the implementation for it, I can&amp;rsquo;t tell). The first example is that it keeps calling &lt;code&gt;PlaceItem()&lt;/code&gt; even though there&amp;rsquo;s no object with that name (and all the &lt;code&gt;please fix error&lt;/code&gt; prompts never saw it touching them). There is however, an object called &lt;code&gt;Place()&lt;/code&gt; which I&amp;rsquo;m assuming is what it was referring to. The second example is where it called &lt;code&gt;updateDay(delete = true)&lt;/code&gt; despite the fact that &lt;code&gt;updateDay()&lt;/code&gt; has a bunch of other required params while it also doesn&amp;rsquo;t have &lt;code&gt;delete&lt;/code&gt; as a param. I can only assume that it just inferred the functionality of the function without actually understanding if it worked as intended.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#ask-clarifying-questions&#34;&gt;
    &lt;h2 id=&#34;ask-clarifying-questions&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Ask clarifying questions&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The prompt I provided is a bit vague to be honest. It&amp;rsquo;s asking to make a UX change without actually providing any design example but rather just describing it with words as if the other person would easily understand it. The LLM went to work immediately with that prompt without asking for more clarifying questions like how the screens get triggered, how the layout should work, how the UI should look etc. I think if LLMs can learn to ask clarifying questions, it can be invaluable for situations like this where the ask might be a little too vague to work off of.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#phenomenal-auto-complete-machine&#34;&gt;
    &lt;h2 id=&#34;phenomenal-auto-complete-machine&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Phenomenal auto-complete machine&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I&amp;rsquo;d be remissed if I didn&amp;rsquo;t mention the auto-complete capabilities of AI coding assistants. In short, they are consistently phenomenal especially when it comes to boilerplate code needing minor tweaks here and there. The AI would make the necessary tweaks automatically making it a breeze when going through the more mind-numbing part of the code base. This is a consistent experience both when I was at Meta (using CodeCompose) and now using Windsurf for my personal project.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#is-it-a-mid-level-engineer-yet&#34;&gt;
    &lt;h2 id=&#34;is-it-a-mid-level-engineer-yet&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Is it a mid-level engineer yet?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Short answer, no. Long answer, it depends. In terms of raw coding ability in an isolated environment, I think it&amp;rsquo;s meeting the mid-level engineer mark just fine (maybe even better due to its breadth generally uncommon among &amp;ldquo;humans&amp;rdquo; lol) but it&amp;rsquo;s the &lt;em&gt;everything else&lt;/em&gt; part that&amp;rsquo;s an issue. For starters, I expect a mid-level engineer to ask for help instead of mindlessly trying to commit code (or send out PRs) over and over again that isn&amp;rsquo;t compiling. I also don&amp;rsquo;t (usually) have to nudge them that their code isn&amp;rsquo;t compiling or failing tests. They can see it themselves and would go work on debugging and fixing them proactively. This is on top of all the issues mentioned above when working in a &lt;em&gt;not-even-that-large&lt;/em&gt; of a codebase.&lt;/p&gt;
&lt;p&gt;For now though, it seems like it&amp;rsquo;s still not good enough to take over even just the coding part of my job so I guess I&amp;rsquo;m going back to implementing the new &lt;code&gt;navhost&lt;/code&gt; for my Android app by myself.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiment Review Process</title>
      <link>https://binhong.me/blog/2025-06-27-experiment-review-process/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-27-experiment-review-process/</guid>
      <description>&lt;p&gt;Mature growth teams would organize a centralized experiment review meeting as a way to share learnings to a wider audience, consult for feedback / next step recommendations, while also holding engineers accountable for the changes they are attempting to ship. The review sessions should be open to anyone to sign up (presenting their experiments) or to participate in general. However, key decision makers (like senior growth engineers, product managers, designers, data scientists) should be required to attend so decisions to ship / not ship / iterate will be made with everyone&amp;rsquo;s concerns addressed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#understanding-the-metric-shifts&#34;&gt;
    &lt;h2 id=&#34;understanding-the-metric-shifts&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Understanding the metric shifts&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When presenting (and reviewing) an experiment, not only do we look at metric shifts but we need to also make sense of them. Understanding them helps inform making better future changes and iterations while ensuring that this change is working as intended instead of some potential regressions on metric blind spots. This usually means that metric analysis for experiments would go beyond just top-line goal / guardrail metrics but also onto regular impression / click loggings to make sure any underlying funnel shifts make sense.&lt;/p&gt;
&lt;p&gt;As an example, when you add stories to the top of your app, you&amp;rsquo;d see an increase in stories traffic. Make sure to also verify that users are actually clicking on the newly added stories component. At the same time, you might be sacrificing screen time on other parts of your app (especially the feature that used to occupy that screen real estate) so you should also see some drop on that end.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#atomic-changes&#34;&gt;
    &lt;h2 id=&#34;atomic-changes&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Atomic changes&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is a bit of an extension from the previous point. It&amp;rsquo;s not uncommon for someone inexperienced to test an overly complex change at one go instead of breaking it into a list of smaller incremental changes. In order to understand which change moved which metric, you need to properly isolate your changes into individual groups (or mix-and-match them with multiple test group setups). This allows you to not only understand what works and what doesn&amp;rsquo;t, but also how each of these different changes might have affected one another. If someone brings in an experiment that&amp;rsquo;s overly complex, you should feel comfortable asking &amp;ldquo;why can&amp;rsquo;t it be smaller (or be run with more test groups)?&amp;rdquo;.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#conflict-of-interest&#34;&gt;
    &lt;h2 id=&#34;conflict-of-interest&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Conflict of interest&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Last week in &lt;a href=&#34;https://binhong.me/blog/2025-06-20-growth-engineer/&#34;&gt;Growth Engineer&lt;/a&gt; we talked a little bit about how sometimes engineers would fall into the trap of looking solely at metrics, over-optimizing them. This is generally good for the engineers themselves (being able to boast larger numbers on their launch) but bad for the business (since they are unlikely to be sustainable long term and will probably regress over time). The role of an experiment review process is to ensure that underlying risks or concerns like this get called out and taken into consideration for ship decisions. In the next few paragraphs, we will explore a few examples of such conflict of interest.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#p-hacking&#34;&gt;
    &lt;h2 id=&#34;p-hacking&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;p hacking&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Usually intentional but sometimes oblivious to the underlying logic, an engineer might participate in p hacking that makes the experiment look better than it really is. One of the most obvious examples would be when someone picks a weirdly specific date range to pull their metrics instead of the more logical (like 1 / 2 week average) standard range. While most experimentation tools would do the statistical analysis and show you results with 95% / 99% / 99.9% confidence levels, there could still be variance (especially on the absolute value derived from its average). On a large change (or accumulation of a lot of smaller changes), this can amount to a rather significant amount.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#long-term-holdout&#34;&gt;
    &lt;h2 id=&#34;long-term-holdout&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Long term holdout&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On some occasions, the effect of a change (especially negative effects) can only be observed over a longer period of time thus making it important to set up a long-term holdout to better understand the longer-term impact of the change. This generally requires some level of product sense and experience to notice thus we would heavily rely on experienced members in the org to call this out. One common example would be &amp;ldquo;user intent&amp;rdquo; (which was previously discussed more in-depth &lt;a href=&#34;https://binhong.me/blog/2025-06-20-growth-engineer/#user-intent&#34;&gt;here&lt;/a&gt;) where long-term behavior of users will better reflect a user&amp;rsquo;s intent as they adopt new habits towards the product change. The other important part of this (often overlooked) process is to follow up on how the long-term holdout performed. It&amp;rsquo;s not uncommon for teams to launch with a holdout then completely forget about the existence of a holdout (especially if the team or product space ended up going through reorgs) leading to a group of poor users still stuck with the sub-optimal experience.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;A strong experiment review process acts as both a learning forum and a quality gate, ensuring experiments are properly analyzed and potential conflicts of interest are surfaced before they impact users. The goal isn&amp;rsquo;t to slow down shipping, but to ship smarter by using the collective experience in the room to catch blind spots that individuals might miss.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Growth Engineer</title>
      <link>https://binhong.me/blog/2025-06-20-growth-engineer/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-20-growth-engineer/</guid>
      <description>&lt;p&gt;While I&amp;rsquo;ve shipped a lot of growth wins (literally the first line on my resume), I&amp;rsquo;m actually very far from a prototypical growth engineer. That said, in this piece, I want to explore a bit more into what it&amp;rsquo;s like being a growth engineer and what makes you good at being one. Growth engineers are generally 1 -&amp;gt; 100 experts instead of 0 -&amp;gt; 1. They fine-tune every little detail by running a lot of experiments with marginal changes to understand the user problem and drive growth impact (&lt;em&gt;line goes up&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#ab-test-everything&#34;&gt;
    &lt;h2 id=&#34;ab-test-everything&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;A/B Test Everything&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When I say &lt;em&gt;everything&lt;/em&gt;, I mean &lt;strong&gt;everything&lt;/strong&gt;. Every button change, content change, margin change, slap an experiment on it. You need to be numbers obsessed and understand how anything moves the topline metric (the metric you / your team cares the most about). In that same sense, you might want to run experiments in chunks of small changes instead of a big chunk to better understand how each small change affects the user behavior. This would later help you in better understanding what is a good growth lever vs what isn&amp;rsquo;t. Alternatively (depending on the type of changes), you can also have multiple variants with different mixes to achieve something similar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For more on A/B testing, check out &lt;a href=&#34;https://binhong.me/blog/2025-06-06-a-b-testing/&#34;&gt;this other post I wrote a few weeks ago here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#understanding-scale&#34;&gt;
    &lt;h2 id=&#34;understanding-scale&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Understanding Scale&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Scale decides necessity and value. If you have no scale, then there&amp;rsquo;s nothing at the top of your funnel to begin with, which makes optimizing the rest of the funnel a common &amp;ldquo;preemptive optimization&amp;rdquo; behavior. Don&amp;rsquo;t add complexity in places you don&amp;rsquo;t need just because &amp;ldquo;it&amp;rsquo;ll be useful in the future&amp;rdquo;. You can build them in the future when needed, then actually measure the value of such a feature to validate your hypothesis (more on this later about A/B testing).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#identifying-opportunities&#34;&gt;
    &lt;h2 id=&#34;identifying-opportunities&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Identifying opportunities&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I wrote &lt;a href=&#34;https://binhong.me/blog/2025-06-13-product-growth-opportunities/&#34;&gt;a separate complementary piece on this a few weeks ago here&lt;/a&gt; mainly because it ended up being so long that it probably deserves to be its own separate thing. This will be a core part of your job and in my opinion the biggest separation factor for a &lt;em&gt;&amp;ldquo;pure bred&amp;rdquo;&lt;/em&gt; (lol) growth engineer. Many engineers get stuck or move away from growth over time (especially around IC5 -&amp;gt; IC6 level) as they realize that they can no longer come up with new ideas and directions to continue growing the product sustainably.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#cost-analysis&#34;&gt;
    &lt;h2 id=&#34;cost-analysis&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Cost analysis&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On some occasions, there will be cost tradeoffs. Most commonly, by spending more money you can get more users (promotions, SMS cost, etc.) which makes it very important to define and understand how much spending is reasonable to earn how many users (cost per user). From there, you should only target to ship projects where it shows that the &lt;em&gt;cost per user&lt;/em&gt; is lower than the given guideline. Important to note that &lt;em&gt;cost&lt;/em&gt; here isn&amp;rsquo;t always directly money, but also valuable real estate (prominent position in the app) or attention (push notification, email, messages) which some could also have a hard-cap (on top of the aforementioned &lt;em&gt;cost per user&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/growth.jpg&#34; target=&#34;_blank&#34;&gt;
    &lt;img src=&#34;https://binhong.me/blog/2025-06-20-growth-engineer//blog/img/growth.jpg&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#user-intent&#34;&gt;
    &lt;h2 id=&#34;user-intent&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;User intent&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;You can&amp;rsquo;t force someone to love you.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When someone (or a team) is metric chasing, the actual user (and their intent) can sometimes be neglected. It&amp;rsquo;s always important to keep them in mind and practice some empathy to better understand what the user actually wants and build around that. Even if you were able to &lt;em&gt;trick&lt;/em&gt; users into something they do not intend to do, the long-term effect of it is likely unsustainable.&lt;/p&gt;
&lt;p&gt;I had a personal counter-example here where I once fixed a 3rd party login issue (after login success, instead of redirecting users back to the 3rd party, we would send them to news feed) resulting in a 2M MAU loss from our test. However, it made sense only because we count every person who sees feed as MAU even when (in this case) they did not intend to do so. We ran a separate long-term experiment and saw that the actual loss was significantly lower than that (like maybe only 5% of the original number) because these people were really just trying to login to a 3rd party app (Spotify, Tinder etc.) instead of wanting to browse news feed.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#failed-tests&#34;&gt;
    &lt;h2 id=&#34;failed-tests&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Failed tests&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When you take a bet and it didn&amp;rsquo;t materialize, your instinct might be to lay low and quietly do away with the project entirely. That would be a massive mistake. Instead, you should focus on what learnings or takeaways you have that can be shared with everyone else &lt;em&gt;especially for someone who might want to try this again in the future&lt;/em&gt;. Generally, there are some key factors on failure root-cause. Make sure to clearly articulate them as you announce your experiment result for the purpose of having clear future reference. If / when someone were to try this again, they can look back at your attempt and see if the landscape had changed enough (based on your learnings) that it might warrant a new attempt.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#long-term-impact&#34;&gt;
    &lt;h2 id=&#34;long-term-impact&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Long term impact&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On one hand, everyone hated the idea of a long-term holdout (where a certain group of users will just have missing features for an extended period of time); on the other hand, you need a way to measure long-term impact of your changes. That said, due to lack of commonality in this practice, the accuracy of such measurement may not be as high as regular shorter-term tests.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Long term impact is a myth - some E6+ Growth Engineer&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The timing makes things worse. By the time you see results from a long-term experiment, you&amp;rsquo;re usually in a different performance review cycle than when you built it. This makes it nearly impossible to properly reward thoughtful long-term thinking or penalize short-sighted decisions. What should be a performance measurement tool becomes just a way to look back and say &amp;lsquo;oh, that&amp;rsquo;s interesting&amp;rsquo; after it&amp;rsquo;s too late to matter.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Growth engineering is fundamentally about disciplined curiosity. You&amp;rsquo;re constantly asking &amp;ldquo;what if we change this?&amp;rdquo; and then actually finding out through experiments rather than debates. It&amp;rsquo;s the difference between having opinions about user behavior and having data about user behavior. Always remember that growth isn&amp;rsquo;t &lt;em&gt;just&lt;/em&gt; about running A/B tests. It&amp;rsquo;s important to also understand how / why users behave a certain way. The challenges are often less &lt;em&gt;technically complex&lt;/em&gt; but more about measurement accuracy, experiment design, and building user-friendly interfaces that even your grandma can easily understand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Product Growth Opportunities</title>
      <link>https://binhong.me/blog/2025-06-13-product-growth-opportunities/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-13-product-growth-opportunities/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s never easy to come up with new ideas that help with growth, but identifying the &lt;em&gt;problem&lt;/em&gt; makes it easier. You&amp;rsquo;ll notice that for the most part in this piece, I&amp;rsquo;ll talk about &lt;em&gt;&amp;ldquo;where&amp;rdquo;&lt;/em&gt; the opportunities are instead of &lt;em&gt;&amp;ldquo;what&amp;rdquo;&lt;/em&gt; because that&amp;rsquo;s usually very domain specific and highly depends on the type of problem you ended up needing to solve.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#funnel-analysis&#34;&gt;
    &lt;h2 id=&#34;funnel-analysis&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Funnel analysis&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The first thing you learn about identifying opportunities in growth is funnel analysis. Build a funnel logging to understand the user journey and go through it to see where you have the biggest drop. From there, you can take a deeper dive into the flow to understand &lt;strong&gt;why&lt;/strong&gt; it is the way it is and see if it can be improved (or if there&amp;rsquo;s any bugs to fix). Similarly, you can go through bug reports to find patterns on how users are being stuck on a specific part of your flow that might have contributed to the funnel losses.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#product-experience&#34;&gt;
    &lt;h2 id=&#34;product-experience&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Product Experience&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Alternatively, you can go straight to the bug reports to see if there are any obvious blocking issues. From there, you can test the fix with an experiment and eventually ship them. Even when it doesn&amp;rsquo;t yield material impact, the changes are generally safe to ship (unless it caused unexpected regressions) since they are largely considered &amp;ldquo;bug fixes&amp;rdquo;. Aside from that, there are other common issues like app performance, screen load time that have a diminishing return curve in user impact.&lt;/p&gt;
&lt;p&gt;My favorite example here was when Uber was working on an iOS rewrite, while everyone was debating about how cellular download limit matters (or not), &lt;a href=&#34;https://x.com/StanTwinB/status/1336929240516710400&#34;&gt;a data scientist pulled together an experiment to show its material impact&lt;/a&gt; (full story &lt;a href=&#34;https://threadreaderapp.com/thread/1336890442768547845.html&#34;&gt;here&lt;/a&gt; - which is one of my favorite software war stories). Similarly, your team should design and run experiments to understand how certain product experiences can have material user impact (instead of just theorizing over them).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#weve-tried-that-before&#34;&gt;
    &lt;h2 id=&#34;weve-tried-that-before&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;&amp;ldquo;We&amp;rsquo;ve tried that before&amp;rdquo;&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Often times, new people will join and suggest &lt;em&gt;old&lt;/em&gt; ideas as new. When that happens, it&amp;rsquo;s easy to dismiss the idea by citing that something similar has been tried previously and didn&amp;rsquo;t yield the expected result. The hard (and valuable) thing here, however, would be to understand why it failed previously and if anything has materially changed since then that might now allow this to be a viable idea to be re-attempted. You might still fail, but you will likely learn something new instead of learning the same old lesson again (which would be a waste of everyone&amp;rsquo;s time and resources).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#studying-your-competitors&#34;&gt;
    &lt;h2 id=&#34;studying-your-competitors&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Studying your competitors&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Sometimes you might find a specific step that has problems (where you&amp;rsquo;re losing a lot more conversion than anywhere else) but you can&amp;rsquo;t figure out how to fix it. Studying how your competition does it can help you see if there&amp;rsquo;s a better way to do that. This doesn&amp;rsquo;t mean their solution would perfectly fit your problem, but it might be worth testing to see if that solution is similarly applicable for you. Alternatively, they might also be struggling with the same issue and what you&amp;rsquo;re both doing is (at least for now) the best solution that anyone has thought of so far. I listed this a lot lower intentionally because it&amp;rsquo;s generally not been &lt;em&gt;that&lt;/em&gt; valuable of an option compared to the other ways of identifying new opportunities above.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#survivorship-bias&#34;&gt;
    &lt;h2 id=&#34;survivorship-bias&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Survivorship Bias&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I wrote this section title with the &amp;ldquo;WWII plane with red dots&amp;rdquo; image in mind. In case you aren&amp;rsquo;t familiar with it, &lt;a href=&#34;https://claude.ai/share/7d06cfc9-95e7-4474-aae4-b56a6c6d3c99&#34;&gt;here&amp;rsquo;s Claude describing it&lt;/a&gt;. In a similar sense, it can be very easy to fall into the trap of focusing on the demand of your existing users instead of building for new users you have yet to (and want to) acquire. Not to say that existing user feedback is unimportant (which I&amp;rsquo;ve literally just said the opposite previously), but rather that it&amp;rsquo;s important to understand and distinguish which feedback helps keep your existing users happy while which feedback is preventing more users from adopting your product.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Finding growth opportunities isn&amp;rsquo;t about hunting for silver bullets, but more about understanding where your product is bleeding users and why. Don&amp;rsquo;t let growth pursuits distract from building something people actually want. The best growth &lt;em&gt;&amp;ldquo;hack&amp;rdquo;&lt;/em&gt; is still making a good product that gets people what they want.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Testing</title>
      <link>https://binhong.me/blog/2025-06-06-a-b-testing/</link>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-06-a-b-testing/</guid>
      <description>&lt;p&gt;This is a basic introduction on how to run a good A/B test. A/B testing is a method where your user pool is segmented into multiple groups, allowing you to test different product interactions and understand how these changes affect user behavior. For any metric / data driven team, A/B testing serves as a critical tool in measuring success.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#tooling&#34;&gt;
    &lt;h2 id=&#34;tooling&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Tooling&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;If your employer does not already have a working tool for experimentation, I&amp;rsquo;d recommend adopting one of the existing tools / platforms (like &lt;a href=&#34;https://statsig.com/&#34;&gt;Statsig&lt;/a&gt;, &lt;a href=&#34;https://www.growthbook.io/&#34;&gt;GrowthBook&lt;/a&gt;, &lt;a href=&#34;https://launchdarkly.com/&#34;&gt;LaunchDarkly&lt;/a&gt; etc.) specifically built with this in mind. On a basic level, they should provide a toggle that splits users into 2 (or more) groups for testing purposes. Their downstream metrics (after first exposure) will then be used to measure if / how these changes might have affected the users. While it&amp;rsquo;s &lt;em&gt;possible&lt;/em&gt; to build your own A/B testing tool in-house, the build and maintenance cost is likely not worth it compared to just adopting ready-made ones.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#goal--guardrail-metrics&#34;&gt;
    &lt;h2 id=&#34;goal--guardrail-metrics&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Goal + Guardrail metrics&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;When a measure becomes a target, it ceases to be a good measure&amp;rdquo; - Goodhart&amp;rsquo;s Law&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Goal metrics serve as a target, the specific thing you want to change or improve. While I largely agree with Goodhart&amp;rsquo;s Law, I also believe that a sufficiently comprehensive &lt;strong&gt;set&lt;/strong&gt; of metrics will help mitigate the downside (or at least minimize it) thus making it still &lt;em&gt;relatively&lt;/em&gt; &amp;ldquo;a good measure&amp;rdquo;. This is where guardrail metrics come in to ensure we aren&amp;rsquo;t just &amp;ldquo;sacrificing x to boost y&amp;rdquo; especially in an unsustainable manner.&lt;/p&gt;
&lt;p&gt;As an example, if you want to increase usage on your app, you can provide generous discounts (or even pay users to do so) but that&amp;rsquo;s obviously very unsustainable so you should set a clear budget on how much you can spend on these promotions and / or cost per new active user acquired you allow for such a project to ship. Here your goal metric would be &amp;ldquo;new active users acquired&amp;rdquo; while your guardrail would be cost (lost revenue). They need to both look good from the experiment before you decide on shipping the product change.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#size-and-duration&#34;&gt;
    &lt;h2 id=&#34;size-and-duration&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Size and Duration&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Generally, size and duration are the 2 main levers to help get your experiment sufficient exposure that allows you to make (statistically) informed decisions. If you don&amp;rsquo;t have a large user base then many of your tests would need a large test group with long test duration to show any statistically significant movement. This unfortunately means that your new feature release cycle will be much slower as your experiments will be queued. In this scenario, you will need to smartly pair up different features that complement one another into the same experiment.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#holdout&#34;&gt;
    &lt;h2 id=&#34;holdout&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Holdout&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In order to accurately measure your team&amp;rsquo;s impact over a certain period of time, you can create a team-wide holdout that keeps a small group of users without the new enhancements. From here, you can see the value of all your team&amp;rsquo;s projects over that period of time (quarter / half) collectively. When you run experiments, you are only testing the impact of specific changes while here you are testing them collectively. Some changes conflict with each other while others complement each other. Ideally you catch them early and run variations of different combinations but you don&amp;rsquo;t always catch them all so having a team wide holdout helps with tracking that.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#novelty-effect&#34;&gt;
    &lt;h2 id=&#34;novelty-effect&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Novelty effect&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When a new restaurant first opens in a popular area, it will be filled with people lining up to try it out. These people will then attract more people who also decide to check out the restaurant out of curiosity. However, the crowd will slowly wind down over time as it loses its &lt;em&gt;novelty&lt;/em&gt; status. If it&amp;rsquo;s any good, it would still remain popular but unlikely to be as popular as it first opened. This is called novelty effect. When you add a new button to the screen, the user&amp;rsquo;s likelihood to click on it increases due to their curiosity on what it does. You don&amp;rsquo;t want to measure that. Instead, you want to measure the actual traffic (and effect) of it &lt;em&gt;after&lt;/em&gt; the novelty wears down. Just like how a good restaurant will still be popular long after its initial launch, you want your feature to be valuable / impactful even long after it&amp;rsquo;s been launched and users clearly understand what it does.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#security&#34;&gt;
    &lt;h2 id=&#34;security&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Security&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When making a security related change (fix or enhancement), a lot of these concepts become a bit trickier especially for critical issues. Mainly because bad actors can figure out that you are doing an A/B test based on their account ID / device ID / IP address and do targeted exploits on the unprotected test group. This makes it rather a futile effort to understand whatever topline metric movement you want to study of this change on regular benign users but instead, becomes more like opening a new window for exploits.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Running good A/B tests isn&amp;rsquo;t just about having the right tools—it&amp;rsquo;s about building the discipline to ask the right questions and measure what actually matters. The fundamentals covered here will prevent most common pitfalls that lead to misleading results or shipping changes that hurt your product. &lt;strong&gt;A/B testing is a means to an end.&lt;/strong&gt; The best growth teams quickly kill bad ideas and double down on what&amp;rsquo;s working. Start simple, get comfortable with the process, and don&amp;rsquo;t let perfect be the enemy of good—a simple, well-run test is infinitely better than a complex experiment that sits in planning forever.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>No Blame SEV (Incident) Culture</title>
      <link>https://binhong.me/blog/2025-05-30-no-blame-sev-culture/</link>
      <pubDate>Fri, 30 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-30-no-blame-sev-culture/</guid>
      <description>&lt;p&gt;Every time there&amp;rsquo;s a major outage at Meta, the first question I get from friends and family is usually &lt;em&gt;&amp;ldquo;did they fire the person who caused it?&amp;rdquo;&lt;/em&gt; which is where I have to explain this concept of &lt;strong&gt;No Blame SEV Culture&lt;/strong&gt;. Especially for an outage so big that a significant number of users are affected, the &lt;em&gt;individual&lt;/em&gt; causing it likely does not have ill intent and there are likely multiple different processes and systems that failed along the way to get us here in the first place.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#process-over-people&#34;&gt;
    &lt;h2 id=&#34;process-over-people&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Process over People&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When something goes wrong (especially something &lt;em&gt;really catastrophic&lt;/em&gt;), it&amp;rsquo;s usually a combination of both process and people problems. The difference here is that process is more deterministic compared to people. People have off-days, get tired, make mistakes etc. so it&amp;rsquo;s important to have a process (or automated systems) in place to prevent that. This can mean anything from adding more test coverage, lint rules against bad code patterns, and / or more alerts. It is however important to note that they need to &lt;strong&gt;maintain a certain level of quality bar&lt;/strong&gt;. As mentioned in &lt;a href=&#34;https://binhong.me/blog/2025-05-04-push-fearlessly-with-automated-testing/#broken-tests&#34;&gt;the previous article&lt;/a&gt;, flaky / broken tests are tech debt, same goes for noisy lint rules and alerts. Too many noisy lint rules and alerts would lead to engineers disregarding them or adopting a &amp;ldquo;wait-and-see&amp;rdquo; mentality which is not ideal in preventing future outages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/sev_review_trifecta.jpeg&#34; target=&#34;_blank&#34;&gt;
    &lt;img src=&#34;https://binhong.me/blog/2025-05-30-no-blame-sev-culture//blog/img/sev_review_trifecta.jpeg&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#expensive-lesson&#34;&gt;
    &lt;h2 id=&#34;expensive-lesson&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Expensive Lesson&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;One of the more interesting quotes I&amp;rsquo;ve read repeatedly (both within and outside of Meta) about people who caused outages is that they just learned an expensive lesson through that specific outage. Firing them (or letting them go) would mean that your company just paid that expensive price of such a lesson for an employee without actually benefiting from it. This employee will then bring this lesson with them to their next employer who would then benefit from such experience.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#fear&#34;&gt;
    &lt;h2 id=&#34;fear&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Fear&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;One of the more significant downsides of &lt;em&gt;blame&lt;/em&gt;, is that you now instill fear in making any sort of production changes (even calculated ones). Instead, it&amp;rsquo;s important to keep in mind that as your product / infra grows, so should your process. Having strong fear in taking any responsibility for even attempting to improve or make fundamental changes breeds complacency. This can be fine in certain organizations and products (like government software, health tech etc.) where there&amp;rsquo;s almost no tolerance for any sort of outages. That said, this is where you should have good chaos engineering and fail-safe practices to ensure the resiliency of your system.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#no-blame--no-responsibility&#34;&gt;
    &lt;h2 id=&#34;no-blame--no-responsibility&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;No Blame ≠ No Responsibility&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is a bit of an exception or outlier effect (and likely the most controversial part of this whole piece). Usually when you cause a really major outage (or multiple for that matter), it&amp;rsquo;s really not your fault (or shouldn&amp;rsquo;t be). But sometimes, smaller outages are understandably less &amp;ldquo;well protected&amp;rdquo; because we expect people to still &lt;em&gt;care&lt;/em&gt; about the things they work on. If you &lt;strong&gt;continuously&lt;/strong&gt; cause outages due to &lt;strong&gt;recklessness&lt;/strong&gt; (&amp;ldquo;lack of &lt;em&gt;care&lt;/em&gt;&amp;rdquo;) especially within a short period of time, you should still be held accountable for it. It&amp;rsquo;s especially common when someone chases the topline metrics movement against a tight timeline (end of a performance review cycle). This does not mean that people should be finger-pointing during the incident review, as before, that should be used to focus on what could&amp;rsquo;ve been better instead. However, it should be brought up separately as part of the performance conversation. It&amp;rsquo;s important to note that this is a scenario where a &lt;em&gt;quantitative change leads to a qualitative change&lt;/em&gt; since an increase in quantity (of incidents) leads to a change in narrative thus should not be used to penalize those who&amp;rsquo;ve only caused one (or maybe two) incidents in a given period of time.&lt;/p&gt;
&lt;p&gt;Aside from that, if there isn&amp;rsquo;t any runbook or recovery plan prepared ahead of time (especially for predictable issues - &lt;em&gt;*subjective*&lt;/em&gt;), it demonstrates a lack of good planning and foresight into the feature. This is - frankly - a lack of competence and the project owner should take responsibility for the rather &lt;em&gt;incomplete&lt;/em&gt; launch. However, the reality is that many individuals would launch buggy projects, claim credit for all the good it brings, while oncalls (spread across the team) pay for the lack of implementation quality. This is especially true when they immediately switch teams after project launches and no longer have to maintain or deal with the aftermath of their uninspiring launch.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;No blame culture means that you aren&amp;rsquo;t fully responsible just because you accidentally touched the house of cards causing it to collapse. Instead, we need better protection around it - like building a fence around it, using LEGO blocks instead of cards, etc - to make sure it doesn&amp;rsquo;t break down easily again after someone accidentally touches it, or just prevent people from accidentally touching it altogether. This means we hold those who are responsible for ensuring the protection accountable instead of those who inevitably discovered the problem. It&amp;rsquo;s like how we don&amp;rsquo;t blame white hat hackers for discovering an exploit; we pay them bounties as a way to thank them for discovering them. We should thank those who found holes in our system&amp;rsquo;s reliability instead of penalizing them for finding it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Code Review Culture</title>
      <link>https://binhong.me/blog/2025-05-23-code-review-culture/</link>
      <pubDate>Fri, 23 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-23-code-review-culture/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Ask a programmer to review 10 lines of code, he&amp;rsquo;ll find 10 issues. Ask him to do 500 lines and he&amp;rsquo;ll say it looks good. - &lt;a href=&#34;https://x.com/girayozil/status/306836785739210752&#34;&gt;@girayozil&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Code review is a really subjective thing where each team or even individual runs things very differently. However, a bad code review process can lead to bad code smells and unnecessary tech debt (just ask all the vibe coders out there 🫣). I will try my best to share my &lt;em&gt;rather opinionated&lt;/em&gt; takes while explaining the reasoning behind each of them.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#overly-prescriptive-formatters&#34;&gt;
    &lt;h2 id=&#34;overly-prescriptive-formatters&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Overly prescriptive formatters&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I&amp;rsquo;m highly supportive of having a preset of an overly prescriptive formatter. It doesn&amp;rsquo;t matter if it&amp;rsquo;s spaces vs tabs, 2 spaces vs 4 spaces, curly braces on the same line or newline; as long as you have a formatter that keeps it consistent. This cuts down on all the unnecessary time reviewing and fixing code formatting while still being easy to read and scan through. Make them as strict as possible, ideally 100% reproducible where every line of code only has one way it can be formatted. Again, it doesn&amp;rsquo;t matter which style you pick but just pick something and stick with it, it&amp;rsquo;s all about consistency here.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#bias-against-newbies&#34;&gt;
    &lt;h2 id=&#34;bias-against-newbies&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Bias against newbies&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;There was some research that finds that when someone is new to the stack / team / industry, reviewers tend to be stricter and more nitpicky against their code changes relative to a long-timer or a more senior engineer. Personally, I see this as a feature instead of a bug. A long-timer has earned their reputation (assuming that&amp;rsquo;s how they are a long-timer and not just a bad engineer successfully avoiding accountability at every turn) and can be afforded to be treated with more leniency in their code. A &lt;em&gt;newbie&lt;/em&gt; however benefits from stricter code review feedback teaching them the style and tradeoffs the team values when it comes to coding patterns, thus learning good practices along the way.&lt;/p&gt;
&lt;p&gt;In an ideal world, teams would have detailed style guides on how everything is written and every new person will be given ample time to read and understand them as part of onboarding. But the reality is that this rarely happens between the outdated style guides and the lack of proper onboarding process, so code review frequently becomes the first time a new person has the opportunity to learn about a team&amp;rsquo;s style guide (aside from linters and formatters). Let&amp;rsquo;s make sure to get it right so they don&amp;rsquo;t develop the wrong habits.&lt;/p&gt;
&lt;p&gt;That said, it&amp;rsquo;s important to note that senior engineers should still hold themselves to a higher level in producing quality code as a way to lead by example. It will be rather unconvincing to a newbie if their reviewer writes &lt;em&gt;subpar code&lt;/em&gt; while holding others to a higher quality bar.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#ask-for-further-details&#34;&gt;
    &lt;h2 id=&#34;ask-for-further-details&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Ask for further details&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Sometimes the change request itself is lacking details about the project or the reason behind such changes. Always ask for further details. In many occasions, it&amp;rsquo;s just the author overlooking it or making an assumption that their reviewers share the same level of context the author has about the project. This is a bit of an extension from the previous section but if you are accepting without understanding &lt;em&gt;why&lt;/em&gt;, you are in a sense &amp;ldquo;blindly accepting&amp;rdquo; the change (even if in this case, it&amp;rsquo;s just &lt;em&gt;partially&lt;/em&gt; blind lol).&lt;/p&gt;
&lt;p&gt;Similarly, if the change is too complex (or if it&amp;rsquo;s bundling too many changes at once), ask for it to be broken down into atomic changes for better / easier review. Having smaller changes also allows for each change to be better reviewed by different domain experts instead of everyone crowding on the same change request. (I understand this is not always feasible so there will be exceptions but ideally, this is done whenever possible.)&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#never-blindly-accept--accept-to-unblock&#34;&gt;
    &lt;h2 id=&#34;never-blindly-accept--accept-to-unblock&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Never blindly accept / accept to unblock&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;These are generally two separate scenarios but their result is generally the same. A change that is otherwise not meeting the code review bar is getting accepted and pushed to production. Blind acceptance generally reflects more of a bad team culture where people either don&amp;rsquo;t take code review seriously or they don&amp;rsquo;t feel comfortable pushing back in code review, both of which are bad. &amp;ldquo;Accept to unblock&amp;rdquo; generally comes with good intention but doing so essentially voids the role of a reviewer. The most common reason for its use is due to reviewers not feeling comfortable about blocking a change but not wishing to take responsibility if that ends up being the bad commit causing an outage.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;only&lt;/strong&gt; exception here would be when it&amp;rsquo;s a &lt;em&gt;time sensitive&lt;/em&gt; change where &amp;ldquo;it can&amp;rsquo;t be worse than it is now&amp;rdquo; &lt;strong&gt;and&lt;/strong&gt; that the author is the &lt;em&gt;domain expert&lt;/em&gt; of the change. The most common example being a code change attempting to mitigate an ongoing production issue. (I&amp;rsquo;ll write up an incident runbook at some point which will most likely include this.)&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#fix-now-not-later&#34;&gt;
    &lt;h2 id=&#34;fix-now-not-later&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Fix &lt;strong&gt;now&lt;/strong&gt; not later&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On some occasions, you might run into a response in your review that they will &amp;ldquo;fix it in a future change&amp;rdquo;. (Generally, it&amp;rsquo;s more common in scenarios where your work adopts the idea of &lt;a href=&#34;https://graphite.dev/guides/stacked-diffs&#34;&gt;&amp;ldquo;stacked diffs&amp;rdquo;&lt;/a&gt;.) This is a bad pattern that should be avoided at large (unless &lt;em&gt;absolutely necessary&lt;/em&gt;) mainly because this change now hinges on a separate future change to be &lt;strong&gt;correct&lt;/strong&gt;, and that the other change could be independently reverted - for any external reasons - without this change, thus causing this bad pattern to now persist in your codebase.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#involve-the-subject-matter-expert&#34;&gt;
    &lt;h2 id=&#34;involve-the-subject-matter-expert&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Involve the Subject Matter Expert&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;If you&amp;rsquo;re tagged / asked to review a change but you think someone else should take a look too, tag them. Most people are very open to reviewing changes when approached directly (especially if they are the SME, the change likely affects them / their team directly too). I&amp;rsquo;ve lost count of the number of times where I wrote some code, pinged the SME, then learned that whatever I&amp;rsquo;m doing is an anti-pattern which &amp;ldquo;works fine now&amp;rdquo; but has no guarantee that it will continue working that way. Funnily, I&amp;rsquo;ve even had an experience where the framework team pulled me in to review changes because I was considered the SME for the use case another person was attempting. In general, it&amp;rsquo;s better to have these discussions &lt;em&gt;before&lt;/em&gt; a change is committed instead of &lt;em&gt;after&lt;/em&gt; (especially if it ends up causing unexpected incidents later on).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#prototyping&#34;&gt;
    &lt;h2 id=&#34;prototyping&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Prototyping&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Code changes that are meant to be prototypes (especially RFCs) should clearly indicate that in their title (or some obvious tags / flags). There are 2 types of prototype code, one which is more of an exploration that is never meant to be shipped; while the other is a recommendation for adoption. In both scenarios, there should be clear indication of which type of prototype code this is and what the intention behind the prototype is (what are you trying to achieve?). For the former - exploration that is never meant to be shipped, the change should also clearly mark that it should never be shipped (hidden / unpublished etc.) with little comments all over explaining each of the &amp;ldquo;mocks&amp;rdquo; or &amp;ldquo;overrides&amp;rdquo; done to achieve the exploration. For the latter - recommendation for adoption, the change should be reviewed as strictly as any regular changes (potentially stricter since it&amp;rsquo;s likely coming from a &lt;em&gt;newbie&lt;/em&gt; as defined above).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#long-review-time&#34;&gt;
    &lt;h2 id=&#34;long-review-time&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Long review time&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is more of a polarizing problem where your team either never heard of this problem, or it&amp;rsquo;s something your team is struggling immensely with. In some teams, the lack of proper &amp;ldquo;incentive&amp;rdquo; for doing code review has greatly reduced team velocity in shipping due to time taken for a code change sitting around waiting for it to be reviewed. While I&amp;rsquo;d usually chalk this up as an &amp;ldquo;incentive&amp;rdquo; problem, I personally believe this as a &amp;ldquo;lead by example&amp;rdquo; cultural shift. Usually teams / orgs better at this have more &lt;em&gt;technically driven&lt;/em&gt; high level ICs who do a lot of (strict?) code reviews themselves, thus fostering a good code review culture throughout their team / org. So in that sense, &amp;ldquo;be the change you want to see&amp;rdquo;.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The culture of code review is something very intentional and will take effort to both cultivate and maintain. The value of good culture however, is usually not something directly measurable (will write a separate piece about intangibles in the future). Having bad culture generally isn&amp;rsquo;t something immediately obvious nor is it something that can be turned around overnight, but rather something that&amp;rsquo;s simmered over a long period of time until it&amp;rsquo;s boiling, by then it&amp;rsquo;s &lt;em&gt;too late&lt;/em&gt; and people are leaving in droves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Art of Posting</title>
      <link>https://binhong.me/blog/2025-05-16-art-of-posting/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-16-art-of-posting/</guid>
      <description>&lt;p&gt;This piece is specifically more about Meta&amp;rsquo;s &lt;em&gt;unique&lt;/em&gt; culture of &amp;ldquo;posting&amp;rdquo; about things but I imagine it can be a useful reference on communication in general. Many people loathe this process and complain about how it&amp;rsquo;s just a lot of &amp;ldquo;self-promotion&amp;rdquo; and while there&amp;rsquo;s definitely some truth to it, it&amp;rsquo;s also a valuable communication avenue that you can leverage to your advantage. As we will explore further below, &lt;em&gt;posting&lt;/em&gt; by itself is more of a bonus as the post will still require actual substance (which is &lt;em&gt;your work&lt;/em&gt; lol).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#proposing-an-idea&#34;&gt;
    &lt;h2 id=&#34;proposing-an-idea&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Proposing an idea&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;A common theme of complaints you might see in a work environment is how &lt;em&gt;someone else&lt;/em&gt; steals your idea. Making a post serves as a paper-trail on this being &lt;em&gt;your idea&lt;/em&gt; as the original author. It&amp;rsquo;s also a useful cross-referencing tool when you run into discussions where your (previously suggested) idea could be a solution, you can share a link instead of going through your idea all over again. By itself, it also serves as a valuable RFC where others are free to comment with their concerns and / or how they feel about things in general. From there, you could gather feedback or even retool your idea a little to eventually become a valuable proposal and contribution towards your team goal.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#tracking-progress&#34;&gt;
    &lt;h2 id=&#34;tracking-progress&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Tracking progress&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When working on a long running project (especially one involving many different people in different stages), posts serve as a good way to track and announce progress, signaling to others if / when they need to start getting involved or otherwise. It&amp;rsquo;s also a valuable communication tool where others can keep track of your project if it ended up affecting their own project (intentionally or otherwise). One of the more common &amp;ldquo;behavioral&amp;rdquo; gap issue for an E4 -&amp;gt; E5 promo is where an E4 can quietly work through the project and launch everything themselves while an E5 would communicate this clearly (outwardly) before even starting to ensure that their project does not end up conflicting with the timeline of other partner teams unintentionally.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrapping-up-a-project&#34;&gt;
    &lt;h2 id=&#34;wrapping-up-a-project&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrapping up a project&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Often times (especially on a long running complex project) by the time it comes to &lt;em&gt;finally&lt;/em&gt; closing out the project, everyone is exhausted and you&amp;rsquo;re just glad that &amp;ldquo;it&amp;rsquo;s done&amp;rdquo; and you can finally move on to the next thing. This is frequently a missed opportunity to nicely wrap up the project and show your appreciation towards everyone who helped make it happen. It&amp;rsquo;s like you went through all the trouble picking the perfect gift, understanding which variant fits them best, finding where to get one, but didn&amp;rsquo;t bother to wrap the gift in a wrapper or a nice little gift bag. Nothing wrong with it, and the value of the gift is still good but having a nice wrapper or even just a little ribbon would&amp;rsquo;ve been a cherry on top of the perfect gift. Similarly, making a post wrapping up the launch and sending thanks to everyone who supported the launch goes a long way in terms of both visibility and relationship building.&lt;/p&gt;
&lt;p&gt;Not all projects are straight up successful launches and / or without leftover cleanup work to do. In fact most projects have some level of cleanup work remaining after it&amp;rsquo;s launched. The wrap up post is the best time to clearly lay them out and at least ensure that they are tracked. Similarly, you might have learnings from the project &lt;em&gt;(especially for failed launches)&lt;/em&gt; which can be useful to share for future references if someone were to run into similar issues or to re-attempt this project again in the future.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#calling-out-potential-issues&#34;&gt;
    &lt;h2 id=&#34;calling-out-potential-issues&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Calling out potential issues&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Honestly this section kinda feels like just spelling out &amp;ldquo;regular social interactions&amp;rdquo; but sometimes it just needs to be said. Both as an author or as a reader of a post, you should take the opportunity to raise any potential issue you might be concern about in the comment of the post. As an author, this allows you to essentially crowdsource potential solution to your concern (maybe someone else has a &amp;ldquo;solution looking for a problem&amp;rdquo; lol). As a reader, you might have different perspective that sees potential risks that the author might have missed, or they figured it out but made an assumption that others would know, your question allows them to clarify their thought process or solution to your question.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There are some exception where if you think something is not suitable to be discussed in such an &lt;strong&gt;open&lt;/strong&gt; setting but that&amp;rsquo;s largely up to your own discretion.&lt;/em&gt;&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#pre-read-from-subject-matter-experts&#34;&gt;
    &lt;h2 id=&#34;pre-read-from-subject-matter-experts&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Pre-read from Subject Matter Experts&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Sometimes you have an idea on stuff you might not be super familiar with (or at least not something you&amp;rsquo;re seen as an expert) and that&amp;rsquo;s fine. But it can be valuable to leverage people around you for their expert opinion. It&amp;rsquo;s also a great way to build work relationship as people like to be seen as an expert. I&amp;rsquo;ve largely found most people to be helpful and open to help look at whatever you&amp;rsquo;ve written as long as you have some level of relationship with them (be it working on a previous project together or trauma bonding through SEVs). Having a SME review your post provides credibility to the things you&amp;rsquo;re writing / proposing and helps get your stuff attention from the correct stakeholders.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Admittedly, this is less of an opinion piece and more of a &amp;ldquo;how to&amp;rdquo; (or &amp;ldquo;how I do x&amp;rdquo;) piece. I don&amp;rsquo;t expect this to change minds entirely on your take about how this is just a lot of self-promotion theatrics but I hope to at least provide a framework on how to use this to your own advantage.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Push Fearlessly with Automated Testing</title>
      <link>https://binhong.me/blog/2025-05-04-push-fearlessly-with-automated-testing/</link>
      <pubDate>Sun, 04 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-04-push-fearlessly-with-automated-testing/</guid>
      <description>&lt;p&gt;Conventional wisdom is that writing tests slows you down, because the alternative is that you don&amp;rsquo;t need to write tests. In fact, Meta (then Facebook) was famously lacking of automated test inline with it&amp;rsquo;s famous &amp;ldquo;move fast and break things&amp;rdquo; mantra. However, from what I&amp;rsquo;ve seen, I beg to differ.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#when-to-write-tests&#34;&gt;
    &lt;h2 id=&#34;when-to-write-tests&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;When to write tests?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;If you love something, put a &lt;del&gt;ring&lt;/del&gt; test on it.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I find this quote as a good general guiding principle. You should definitely have tests for all the &amp;ldquo;core&amp;rdquo; features. For all the additional features (or edge case handling), you&amp;rsquo;d need to make a decision / tradeoff based on how critical it is for something to work, and how frequent you might be changing it.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#fearlessly&#34;&gt;
    &lt;h2 id=&#34;fearlessly&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Fearlessly&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The main goal of writing tests is to prevent regressions. Without automated tests, you&amp;rsquo;d have to rely on other (less reliable) ways to avoid regression. Having (non-flaky) tests makes it easy to consistently verify when / how a regression is introduced. With the right CI system setup, you can make sure that you don&amp;rsquo;t release new versions / feature that would accidentally break previously released features in an unexpected way.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#assumptions--prerequisite&#34;&gt;
    &lt;h2 id=&#34;assumptions--prerequisite&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Assumptions / Prerequisite&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The expectation here is that you already have a working CI/CD pipeline setup which comprehensively runs all your tests for each commit merges and deployments. Alternatively, if you have too big of a test suite to run (or too costly), you could instead only run relevant tests on each individual commits while running all tests for each deployment.  If there’s a test that failed before deployment, you could still bisect to find the blame commit that broke the test.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#what-about-manual-testing&#34;&gt;
    &lt;h2 id=&#34;what-about-manual-testing&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;What about manual testing?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The main issue with manual testing is that you don&amp;rsquo;t know what you don&amp;rsquo;t know. You know your code change affects x and y so you test them both and they work as intended but what you didn&amp;rsquo;t realize was that it also breaks z entirely which you did not test because that was unexpected. Having an automated test means you&amp;rsquo;d just always run the test suite (within your CI/CD process) which would be able to catch the issue.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#what-about-qa-testing&#34;&gt;
    &lt;h2 id=&#34;what-about-qa-testing&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;What about QA testing?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;QA testing generally happens on a weekly or bi-weekly (either definition) cadence. At best, you can do them on a daily basis but depending on the scale or speed at which your team is executing, it might still be hard to narrow down the specific commit that is causing the regression. With automated tests, you can bisect through your commit history without having to worry about setup and cleanup steps (assuming your tests are properly written).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#why-not-just-write-test-for-everything&#34;&gt;
    &lt;h2 id=&#34;why-not-just-write-test-for-everything&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Why not just write test for everything?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In an ideal world where the cost to write test is 0, this would be a prescribed solution and everyone will be adopting TDD (Test Driven Development) but instead, we live in a real world where writing tests cost time and energy so it&amp;rsquo;s important to pick and choose things worth investing on vs things that aren&amp;rsquo;t. Such tradeoff is subjective to the criticality of said system and how often the system might be changed / updated.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#broken-tests&#34;&gt;
    &lt;h2 id=&#34;broken-tests&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Broken Tests&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Broken tests are &lt;strong&gt;tech debts&lt;/strong&gt;. Either prioritize fixing them or delete them if they are no longer relevant. Keeping them around gives false impression on the state of the test coverage while potentially adds unnecessary operational cost in running these tests. Same applies to flaky tests. In fact, I&amp;rsquo;d go a step further by saying that flaky tests are worse than broken tests because at least broken tests are deterministic (which is one important characteristic of a quality test).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When you have working quality automated tests, you can push new changes fearlessly knowing that it won&amp;rsquo;t cause regressions on features protected by these tests. This allows you to move faster both in terms of thinking about and testing all the possible regressions your change might cause, while also saving time on root causing a regression (if / when it does happen).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Opinionated Engineer</title>
      <link>https://binhong.me/blog/2025-05-04-the-opinionated-engineer/</link>
      <pubDate>Sun, 04 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-04-the-opinionated-engineer/</guid>
      <description>&lt;p&gt;Historically, my personal blog has been mostly about &amp;ldquo;how to do x&amp;rdquo;, &amp;ldquo;guide on making y&amp;rdquo; but never about ideology or engineering practices. This new series intends to change that where I&amp;rsquo;ll explore the more opinionated side of myself especially around engineering practices instead of just the &lt;em&gt;pure&lt;/em&gt; technical stuff.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#topics&#34;&gt;
    &lt;h2 id=&#34;topics&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Topics&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I have not fully planned out the entire series but here are a few topics I have in mind (or have released) so far&amp;hellip;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Behavioral
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-05-15-art-of-posting/&#34;&gt;The Art of Posting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-09-19-communicating-effectively/&#34;&gt;Communicating Effectively&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-10-17-the-myth-of-technical-complexity/&#34;&gt;They Myth of Technical Complexity&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Culture
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-05-23-code-review-culture/&#34;&gt;Code Review Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-05-04-push-fearlessly-with-automated-testing/&#34;&gt;Push Fearlessly with Automated Testing&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Dev Tools
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-07-03-early-takes-on-vibe-coding/&#34;&gt;Early takes on vibe-coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-07-11-internal-tooling-ideas/&#34;&gt;Internal Tooling Ideas&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-07-18-understanding-value-of-dev-tools/&#34;&gt;Understanding the Value of Dev Tools&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Incident Management
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-05-30-no-blame-sev-culture/&#34;&gt;No Blame SEV (Incident) Culture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-07-25-major-incident-runbook/&#34;&gt;Major Incident Runbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-08-01-firefighting-heroes/&#34;&gt;Firefighting Heroes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Product Growth
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-06-06-a-b-testing/&#34;&gt;A/B Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-06-13-product-growth-opportunities/&#34;&gt;Product Growth Opportunities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-06-20-growth-engineer/&#34;&gt;Growth Engineer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-06-27-experiment-review-process/&#34;&gt;Experiment Review Process&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://binhong.me/blog/2025-08-23-hidden-privacy-and-security-pitfalls/&#34;&gt;Hidden Privacy and Security Pitfalls&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#where-is-this-coming-from&#34;&gt;
    &lt;h2 id=&#34;where-is-this-coming-from&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Where is this coming from?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Back when I was working at Meta, I would generally build an idea proposal (on how / what to fix) around an opinionated piece like this. It largely serves like a RFC where I can gather other (both supporting or opposing) opinions and concerns before proceeding to make sure that I&amp;rsquo;ve considered everyone&amp;rsquo;s perspective. The difference in this series however, I would focus more on exploring my personal opinions and why I feel strongly about them. It&amp;rsquo;s up to you to decide whether it fits the bill for you.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#what-should-i-do-with-this-information&#34;&gt;
    &lt;h2 id=&#34;what-should-i-do-with-this-information&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;What should I do with this information?&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;These are not meant to be blind prescription (especially since your situation could very well be different from what I am / was going through) but rather as an extra perspective for consideration when deciding on relevant matters. As the title states, these are opinions with which you&amp;rsquo;re open to have opposing views.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#feedback&#34;&gt;
    &lt;h2 id=&#34;feedback&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Feedback&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Unfortunately I don&amp;rsquo;t have comment feature setup on here but feel free to respond and tag me on &lt;a href=&#34;https://threads.com/@binhonglee&#34;&gt;Threads&lt;/a&gt; or &lt;a href=&#34;https://social.lol/@bh&#34;&gt;Mastodon&lt;/a&gt;. I&amp;rsquo;m generally active and happy to engage in any discussions and / or learn more about any counter examples!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
