<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Growth on BinHong Lee&#39;s Blog</title>
    <link>https://binhong.me/blog/tags/growth/</link>
    <description>Recent content in Growth on BinHong Lee&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>binhong@binhong.me (BinHong Lee)</managingEditor>
    <webMaster>binhong@binhong.me (BinHong Lee)</webMaster>
    <lastBuildDate>Fri, 03 Oct 2025 00:00:00 -0800</lastBuildDate><atom:link href="https://binhong.me/blog/tags/growth/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Shipping a project with -2M MAP</title>
      <link>https://binhong.me/blog/2025-10-03-shipping-negative-2m-map/</link>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-10-03-shipping-negative-2m-map/</guid>
      <description>&lt;p&gt;There&amp;rsquo;s a saying in Meta that &amp;lsquo;Nothing is somebody else&amp;rsquo;s problem&amp;rsquo; but sometimes, it really is. At this point in time, I was working on a growth team whose top-line goal was to grow Facebook MAP (monthly active people). So naturally, it&amp;rsquo;s surprising for me to have shipped something that directly regressed the very metric the team should be growing.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/project-presentation/&#34;&gt;(Project Presentation)&lt;/a&gt; where I share stories of my past projects.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#background&#34;&gt;
    &lt;h2 id=&#34;background&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Background&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This was still within the first year of me joining the team. Unfortunately, up to this point, I hadn&amp;rsquo;t had any successful (+MAP) launches, and I&amp;rsquo;d been eager to prove myself. The most recent project I worked on before this was to show an educational screen to users right after they logged in (under a specific condition) before they got redirected to newsfeed. Once a user logs an impression on newsfeed, they are considered MAP.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/negative_2m_map_1.png&#34; target=&#34;_blank&#34;&gt;
    
    &lt;img src=&#34;https://binhong.me/blog/2025-10-03-shipping-negative-2m-map//blog/img/negative_2m_map_1.png&#34;&gt;
    
&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;My project failed because for some reason it logged a MAP increase on the very first day - which didn&amp;rsquo;t make sense because users should be MAP on the first day regardless of whether they&amp;rsquo;re in the test or control group. If anything, it could have had a slight drop if users dropped off on the interstitial before they reached newsfeed. At this point, my manager suggested that we move on to something else since no one really knew why. I was, however, adamant about figuring out what happened, so I started digging into it anyway. (&lt;strong&gt;major mistake&lt;/strong&gt;)&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#the-bug&#34;&gt;
    &lt;h2 id=&#34;the-bug&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;The Bug&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I dug through the stacktrace step-by-step, the code line-by-line, until I saw a variable that caught my eye: &lt;code&gt;is_platform_login&lt;/code&gt;. For some reason, when it was true, we ignored the &lt;code&gt;next&lt;/code&gt; param in redirection. I didn&amp;rsquo;t know what &amp;ldquo;platform login&amp;rdquo; was, so it was time to get some help. I asked my mentor / tech lead what &amp;ldquo;platform login&amp;rdquo; was and was told that it&amp;rsquo;s third-party login (think &amp;ldquo;Login with Facebook&amp;rdquo; on other sites). It&amp;rsquo;s something owned by a separate team, and I should go ask this specific person (Kevin) if I had more questions about it. So I decided to test how it worked. I opened up Spotify, clicked &amp;ldquo;Login with Facebook,&amp;rdquo; then logged in (through that specific condition), and thereafter I was redirected to FB Newsfeed instead of going back to Spotify to complete my login!&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#discovering-the-scope&#34;&gt;
    &lt;h2 id=&#34;discovering-the-scope&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Discovering the scope&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;First, I looked up the blame and just asked the person (on a sister team) why they explicitly excluded platform login from this redirection. They told me that&amp;rsquo;s just how the team tests and ships new features in general. Fair, but I didn&amp;rsquo;t think it made sense in this case. So I reached out to Kevin and asked if they were aware of the bug. They were surprised and agreed with my assessment that it should be fixed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/negative_2m_map_2.png&#34; target=&#34;_blank&#34;&gt;
    
    &lt;img src=&#34;https://binhong.me/blog/2025-10-03-shipping-negative-2m-map//blog/img/negative_2m_map_2.png&#34;&gt;
    
&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;I merged the change and started an experiment for the fix. After a while, I pulled the metrics on the experiment and saw that it regressed MAP heavily, around 2M MAP. This wasn&amp;rsquo;t exactly surprising, as you&amp;rsquo;ve seen in the image above, since we were redirecting the users back to the third-party platform source, which no longer counted them as MAP. I went back to Kevin and asked what their team&amp;rsquo;s goal metrics were and if they had a data scientist on their team willing to help with experiment analysis.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#justifying-the-ship&#34;&gt;
    &lt;h2 id=&#34;justifying-the-ship&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Justifying the ship&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;We brought the experiment to be reviewed. (If you aren&amp;rsquo;t familiar, &lt;a href=&#34;https://binhong.me/blog/2025-06-27-experiment-review-process/&#34;&gt;here&amp;rsquo;s a previous article on how experiment review works&lt;/a&gt;.) Most people understood the reasoning behind the ship recommendation, but there were some questions around better understanding whether this was intended user behavior. After some back-and-forth, we agreed to ship this with a long-term holdout to study the potential long-term MAP effect.&lt;/p&gt;
&lt;p&gt;A few months later, we pulled the data for the long-term holdout and verified that the long-term MAP regression was closer to 20k. This meant that 90% of the users who were &lt;em&gt;forced&lt;/em&gt; to newsfeed (caused by the &lt;em&gt;bug&lt;/em&gt;) did not stay MAP for very long and eventually churned one way or another anyway. It&amp;rsquo;s likely because these users did not intent to &lt;em&gt;actually use&lt;/em&gt; Facebook to begin with, thus also unlikely to stick around when they were &lt;em&gt;wrongly&lt;/em&gt; redirected.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#takeaway&#34;&gt;
    &lt;h2 id=&#34;takeaway&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Takeaway&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;For one, I was penalized for essentially spending too much time on work that wasn&amp;rsquo;t aligned with the team I was on. Realistically, I should have taken my manager&amp;rsquo;s advice to move on to another project. Even after I&amp;rsquo;d found the bug, I should have passed this off to Kevin and their team to run the experiment and provided support instead of trying to be the hero. The results from the long-term holdout also didn&amp;rsquo;t materialize until the next performance review cycle, so that definitely didn&amp;rsquo;t help.&lt;/p&gt;
&lt;p&gt;Finally, I still never actually figured out whether this was the bug that was causing my original project to fail. We suspect it might be related, but I never re-ran it and never got any sort of verification that it solved the issue. It was a lot of learning experience for me overall, but at least I delivered (imo) the ideal user experience and was able to justify the change &lt;em&gt;despite&lt;/em&gt; the team&amp;rsquo;s goal metrics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Experiment Review Process</title>
      <link>https://binhong.me/blog/2025-06-27-experiment-review-process/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-27-experiment-review-process/</guid>
      <description>&lt;p&gt;Mature growth teams would organize a centralized experiment review meeting as a way to share learnings to a wider audience, consult for feedback / next step recommendations, while also holding engineers accountable for the changes they are attempting to ship. The review sessions should be open to anyone to sign up (presenting their experiments) or to participate in general. However, key decision makers (like senior growth engineers, product managers, designers, data scientists) should be required to attend so decisions to ship / not ship / iterate will be made with everyone&amp;rsquo;s concerns addressed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#understanding-the-metric-shifts&#34;&gt;
    &lt;h2 id=&#34;understanding-the-metric-shifts&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Understanding the metric shifts&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When presenting (and reviewing) an experiment, not only do we look at metric shifts but we need to also make sense of them. Understanding them helps inform making better future changes and iterations while ensuring that this change is working as intended instead of some potential regressions on metric blind spots. This usually means that metric analysis for experiments would go beyond just top-line goal / guardrail metrics but also onto regular impression / click loggings to make sure any underlying funnel shifts make sense.&lt;/p&gt;
&lt;p&gt;As an example, when you add stories to the top of your app, you&amp;rsquo;d see an increase in stories traffic. Make sure to also verify that users are actually clicking on the newly added stories component. At the same time, you might be sacrificing screen time on other parts of your app (especially the feature that used to occupy that screen real estate) so you should also see some drop on that end.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#atomic-changes&#34;&gt;
    &lt;h2 id=&#34;atomic-changes&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Atomic changes&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is a bit of an extension from the previous point. It&amp;rsquo;s not uncommon for someone inexperienced to test an overly complex change at one go instead of breaking it into a list of smaller incremental changes. In order to understand which change moved which metric, you need to properly isolate your changes into individual groups (or mix-and-match them with multiple test group setups). This allows you to not only understand what works and what doesn&amp;rsquo;t, but also how each of these different changes might have affected one another. If someone brings in an experiment that&amp;rsquo;s overly complex, you should feel comfortable asking &amp;ldquo;why can&amp;rsquo;t it be smaller (or be run with more test groups)?&amp;rdquo;.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#conflict-of-interest&#34;&gt;
    &lt;h2 id=&#34;conflict-of-interest&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Conflict of interest&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Last week in &lt;a href=&#34;https://binhong.me/blog/2025-06-20-growth-engineer/&#34;&gt;Growth Engineer&lt;/a&gt; we talked a little bit about how sometimes engineers would fall into the trap of looking solely at metrics, over-optimizing them. This is generally good for the engineers themselves (being able to boast larger numbers on their launch) but bad for the business (since they are unlikely to be sustainable long term and will probably regress over time). The role of an experiment review process is to ensure that underlying risks or concerns like this get called out and taken into consideration for ship decisions. In the next few paragraphs, we will explore a few examples of such conflict of interest.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#p-hacking&#34;&gt;
    &lt;h2 id=&#34;p-hacking&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;p hacking&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Usually intentional but sometimes oblivious to the underlying logic, an engineer might participate in p hacking that makes the experiment look better than it really is. One of the most obvious examples would be when someone picks a weirdly specific date range to pull their metrics instead of the more logical (like 1 / 2 week average) standard range. While most experimentation tools would do the statistical analysis and show you results with 95% / 99% / 99.9% confidence levels, there could still be variance (especially on the absolute value derived from its average). On a large change (or accumulation of a lot of smaller changes), this can amount to a rather significant amount.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#long-term-holdout&#34;&gt;
    &lt;h2 id=&#34;long-term-holdout&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Long term holdout&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On some occasions, the effect of a change (especially negative effects) can only be observed over a longer period of time thus making it important to set up a long-term holdout to better understand the longer-term impact of the change. This generally requires some level of product sense and experience to notice thus we would heavily rely on experienced members in the org to call this out. One common example would be &amp;ldquo;user intent&amp;rdquo; (which was previously discussed more in-depth &lt;a href=&#34;https://binhong.me/blog/2025-06-20-growth-engineer/#user-intent&#34;&gt;here&lt;/a&gt;) where long-term behavior of users will better reflect a user&amp;rsquo;s intent as they adopt new habits towards the product change. The other important part of this (often overlooked) process is to follow up on how the long-term holdout performed. It&amp;rsquo;s not uncommon for teams to launch with a holdout then completely forget about the existence of a holdout (especially if the team or product space ended up going through reorgs) leading to a group of poor users still stuck with the sub-optimal experience.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;A strong experiment review process acts as both a learning forum and a quality gate, ensuring experiments are properly analyzed and potential conflicts of interest are surfaced before they impact users. The goal isn&amp;rsquo;t to slow down shipping, but to ship smarter by using the collective experience in the room to catch blind spots that individuals might miss.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Growth Engineer</title>
      <link>https://binhong.me/blog/2025-06-20-growth-engineer/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-20-growth-engineer/</guid>
      <description>&lt;p&gt;While I&amp;rsquo;ve shipped a lot of growth wins (literally the first line on my resume), I&amp;rsquo;m actually very far from a prototypical growth engineer. That said, in this piece, I want to explore a bit more into what it&amp;rsquo;s like being a growth engineer and what makes you good at being one. Growth engineers are generally 1 -&amp;gt; 100 experts instead of 0 -&amp;gt; 1. They fine-tune every little detail by running a lot of experiments with marginal changes to understand the user problem and drive growth impact (&lt;em&gt;line goes up&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#ab-test-everything&#34;&gt;
    &lt;h2 id=&#34;ab-test-everything&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;A/B Test Everything&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When I say &lt;em&gt;everything&lt;/em&gt;, I mean &lt;strong&gt;everything&lt;/strong&gt;. Every button change, content change, margin change, slap an experiment on it. You need to be numbers obsessed and understand how anything moves the topline metric (the metric you / your team cares the most about). In that same sense, you might want to run experiments in chunks of small changes instead of a big chunk to better understand how each small change affects the user behavior. This would later help you in better understanding what is a good growth lever vs what isn&amp;rsquo;t. Alternatively (depending on the type of changes), you can also have multiple variants with different mixes to achieve something similar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For more on A/B testing, check out &lt;a href=&#34;https://binhong.me/blog/2025-06-06-a-b-testing/&#34;&gt;this other post I wrote a few weeks ago here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#understanding-scale&#34;&gt;
    &lt;h2 id=&#34;understanding-scale&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Understanding Scale&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Scale decides necessity and value. If you have no scale, then there&amp;rsquo;s nothing at the top of your funnel to begin with, which makes optimizing the rest of the funnel a common &amp;ldquo;preemptive optimization&amp;rdquo; behavior. Don&amp;rsquo;t add complexity in places you don&amp;rsquo;t need just because &amp;ldquo;it&amp;rsquo;ll be useful in the future&amp;rdquo;. You can build them in the future when needed, then actually measure the value of such a feature to validate your hypothesis (more on this later about A/B testing).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#identifying-opportunities&#34;&gt;
    &lt;h2 id=&#34;identifying-opportunities&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Identifying opportunities&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I wrote &lt;a href=&#34;https://binhong.me/blog/2025-06-13-product-growth-opportunities/&#34;&gt;a separate complementary piece on this a few weeks ago here&lt;/a&gt; mainly because it ended up being so long that it probably deserves to be its own separate thing. This will be a core part of your job and in my opinion the biggest separation factor for a &lt;em&gt;&amp;ldquo;pure bred&amp;rdquo;&lt;/em&gt; (lol) growth engineer. Many engineers get stuck or move away from growth over time (especially around IC5 -&amp;gt; IC6 level) as they realize that they can no longer come up with new ideas and directions to continue growing the product sustainably.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#cost-analysis&#34;&gt;
    &lt;h2 id=&#34;cost-analysis&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Cost analysis&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On some occasions, there will be cost tradeoffs. Most commonly, by spending more money you can get more users (promotions, SMS cost, etc.) which makes it very important to define and understand how much spending is reasonable to earn how many users (cost per user). From there, you should only target to ship projects where it shows that the &lt;em&gt;cost per user&lt;/em&gt; is lower than the given guideline. Important to note that &lt;em&gt;cost&lt;/em&gt; here isn&amp;rsquo;t always directly money, but also valuable real estate (prominent position in the app) or attention (push notification, email, messages) which some could also have a hard-cap (on top of the aforementioned &lt;em&gt;cost per user&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/growth.jpg&#34; target=&#34;_blank&#34;&gt;
    
    &lt;img src=&#34;https://binhong.me/blog/2025-06-20-growth-engineer//blog/img/growth.jpg&#34;&gt;
    
&lt;/a&gt;
&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#user-intent&#34;&gt;
    &lt;h2 id=&#34;user-intent&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;User intent&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;You can&amp;rsquo;t force someone to love you.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;When someone (or a team) is metric chasing, the actual user (and their intent) can sometimes be neglected. It&amp;rsquo;s always important to keep them in mind and practice some empathy to better understand what the user actually wants and build around that. Even if you were able to &lt;em&gt;trick&lt;/em&gt; users into something they do not intend to do, the long-term effect of it is likely unsustainable.&lt;/p&gt;
&lt;p&gt;I had a personal counter-example here where I once fixed a 3rd party login issue (after login success, instead of redirecting users back to the 3rd party, we would send them to news feed) resulting in a 2M MAU loss from our test. However, it made sense only because we count every person who sees feed as MAU even when (in this case) they did not intend to do so. We ran a separate long-term experiment and saw that the actual loss was significantly lower than that (like maybe only 5% of the original number) because these people were really just trying to login to a 3rd party app (Spotify, Tinder etc.) instead of wanting to browse news feed.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#failed-tests&#34;&gt;
    &lt;h2 id=&#34;failed-tests&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Failed tests&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When you take a bet and it didn&amp;rsquo;t materialize, your instinct might be to lay low and quietly do away with the project entirely. That would be a massive mistake. Instead, you should focus on what learnings or takeaways you have that can be shared with everyone else &lt;em&gt;especially for someone who might want to try this again in the future&lt;/em&gt;. Generally, there are some key factors on failure root-cause. Make sure to clearly articulate them as you announce your experiment result for the purpose of having clear future reference. If / when someone were to try this again, they can look back at your attempt and see if the landscape had changed enough (based on your learnings) that it might warrant a new attempt.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#long-term-impact&#34;&gt;
    &lt;h2 id=&#34;long-term-impact&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Long term impact&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;On one hand, everyone hated the idea of a long-term holdout (where a certain group of users will just have missing features for an extended period of time); on the other hand, you need a way to measure long-term impact of your changes. That said, due to lack of commonality in this practice, the accuracy of such measurement may not be as high as regular shorter-term tests.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Long term impact is a myth - some E6+ Growth Engineer&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The timing makes things worse. By the time you see results from a long-term experiment, you&amp;rsquo;re usually in a different performance review cycle than when you built it. This makes it nearly impossible to properly reward thoughtful long-term thinking or penalize short-sighted decisions. What should be a performance measurement tool becomes just a way to look back and say &amp;lsquo;oh, that&amp;rsquo;s interesting&amp;rsquo; after it&amp;rsquo;s too late to matter.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Growth engineering is fundamentally about disciplined curiosity. You&amp;rsquo;re constantly asking &amp;ldquo;what if we change this?&amp;rdquo; and then actually finding out through experiments rather than debates. It&amp;rsquo;s the difference between having opinions about user behavior and having data about user behavior. Always remember that growth isn&amp;rsquo;t &lt;em&gt;just&lt;/em&gt; about running A/B tests. It&amp;rsquo;s important to also understand how / why users behave a certain way. The challenges are often less &lt;em&gt;technically complex&lt;/em&gt; but more about measurement accuracy, experiment design, and building user-friendly interfaces that even your grandma can easily understand.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Product Growth Opportunities</title>
      <link>https://binhong.me/blog/2025-06-13-product-growth-opportunities/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-13-product-growth-opportunities/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s never easy to come up with new ideas that help with growth, but identifying the &lt;em&gt;problem&lt;/em&gt; makes it easier. You&amp;rsquo;ll notice that for the most part in this piece, I&amp;rsquo;ll talk about &lt;em&gt;&amp;ldquo;where&amp;rdquo;&lt;/em&gt; the opportunities are instead of &lt;em&gt;&amp;ldquo;what&amp;rdquo;&lt;/em&gt; because that&amp;rsquo;s usually very domain specific and highly depends on the type of problem you ended up needing to solve.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#funnel-analysis&#34;&gt;
    &lt;h2 id=&#34;funnel-analysis&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Funnel analysis&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The first thing you learn about identifying opportunities in growth is funnel analysis. Build a funnel logging to understand the user journey and go through it to see where you have the biggest drop. From there, you can take a deeper dive into the flow to understand &lt;strong&gt;why&lt;/strong&gt; it is the way it is and see if it can be improved (or if there&amp;rsquo;s any bugs to fix). Similarly, you can go through bug reports to find patterns on how users are being stuck on a specific part of your flow that might have contributed to the funnel losses.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#product-experience&#34;&gt;
    &lt;h2 id=&#34;product-experience&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Product Experience&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Alternatively, you can go straight to the bug reports to see if there are any obvious blocking issues. From there, you can test the fix with an experiment and eventually ship them. Even when it doesn&amp;rsquo;t yield material impact, the changes are generally safe to ship (unless it caused unexpected regressions) since they are largely considered &amp;ldquo;bug fixes&amp;rdquo;. Aside from that, there are other common issues like app performance, screen load time that have a diminishing return curve in user impact.&lt;/p&gt;
&lt;p&gt;My favorite example here was when Uber was working on an iOS rewrite, while everyone was debating about how cellular download limit matters (or not), &lt;a href=&#34;https://x.com/StanTwinB/status/1336929240516710400&#34;&gt;a data scientist pulled together an experiment to show its material impact&lt;/a&gt; (full story &lt;a href=&#34;https://threadreaderapp.com/thread/1336890442768547845.html&#34;&gt;here&lt;/a&gt; - which is one of my favorite software war stories). Similarly, your team should design and run experiments to understand how certain product experiences can have material user impact (instead of just theorizing over them).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#weve-tried-that-before&#34;&gt;
    &lt;h2 id=&#34;weve-tried-that-before&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;&amp;ldquo;We&amp;rsquo;ve tried that before&amp;rdquo;&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Often times, new people will join and suggest &lt;em&gt;old&lt;/em&gt; ideas as new. When that happens, it&amp;rsquo;s easy to dismiss the idea by citing that something similar has been tried previously and didn&amp;rsquo;t yield the expected result. The hard (and valuable) thing here, however, would be to understand why it failed previously and if anything has materially changed since then that might now allow this to be a viable idea to be re-attempted. You might still fail, but you will likely learn something new instead of learning the same old lesson again (which would be a waste of everyone&amp;rsquo;s time and resources).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#studying-your-competitors&#34;&gt;
    &lt;h2 id=&#34;studying-your-competitors&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Studying your competitors&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Sometimes you might find a specific step that has problems (where you&amp;rsquo;re losing a lot more conversion than anywhere else) but you can&amp;rsquo;t figure out how to fix it. Studying how your competition does it can help you see if there&amp;rsquo;s a better way to do that. This doesn&amp;rsquo;t mean their solution would perfectly fit your problem, but it might be worth testing to see if that solution is similarly applicable for you. Alternatively, they might also be struggling with the same issue and what you&amp;rsquo;re both doing is (at least for now) the best solution that anyone has thought of so far. I listed this a lot lower intentionally because it&amp;rsquo;s generally not been &lt;em&gt;that&lt;/em&gt; valuable of an option compared to the other ways of identifying new opportunities above.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#survivorship-bias&#34;&gt;
    &lt;h2 id=&#34;survivorship-bias&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Survivorship Bias&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;I wrote this section title with the &amp;ldquo;WWII plane with red dots&amp;rdquo; image in mind. In case you aren&amp;rsquo;t familiar with it, &lt;a href=&#34;https://claude.ai/share/7d06cfc9-95e7-4474-aae4-b56a6c6d3c99&#34;&gt;here&amp;rsquo;s Claude describing it&lt;/a&gt;. In a similar sense, it can be very easy to fall into the trap of focusing on the demand of your existing users instead of building for new users you have yet to (and want to) acquire. Not to say that existing user feedback is unimportant (which I&amp;rsquo;ve literally just said the opposite previously), but rather that it&amp;rsquo;s important to understand and distinguish which feedback helps keep your existing users happy while which feedback is preventing more users from adopting your product.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Finding growth opportunities isn&amp;rsquo;t about hunting for silver bullets, but more about understanding where your product is bleeding users and why. Don&amp;rsquo;t let growth pursuits distract from building something people actually want. The best growth &lt;em&gt;&amp;ldquo;hack&amp;rdquo;&lt;/em&gt; is still making a good product that gets people what they want.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A/B Testing</title>
      <link>https://binhong.me/blog/2025-06-06-a-b-testing/</link>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-06-06-a-b-testing/</guid>
      <description>&lt;p&gt;This is a basic introduction on how to run a good A/B test. A/B testing is a method where your user pool is segmented into multiple groups, allowing you to test different product interactions and understand how these changes affect user behavior. For any metric / data driven team, A/B testing serves as a critical tool in measuring success.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        
&lt;a class=&#34;anchor&#34; href=&#34;#tooling&#34;&gt;
    &lt;h2 id=&#34;tooling&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Tooling&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;If your employer does not already have a working tool for experimentation, I&amp;rsquo;d recommend adopting one of the existing tools / platforms (like &lt;a href=&#34;https://statsig.com/&#34;&gt;Statsig&lt;/a&gt;, &lt;a href=&#34;https://www.growthbook.io/&#34;&gt;GrowthBook&lt;/a&gt;, &lt;a href=&#34;https://launchdarkly.com/&#34;&gt;LaunchDarkly&lt;/a&gt; etc.) specifically built with this in mind. On a basic level, they should provide a toggle that splits users into 2 (or more) groups for testing purposes. Their downstream metrics (after first exposure) will then be used to measure if / how these changes might have affected the users. While it&amp;rsquo;s &lt;em&gt;possible&lt;/em&gt; to build your own A/B testing tool in-house, the build and maintenance cost is likely not worth it compared to just adopting ready-made ones.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#goal--guardrail-metrics&#34;&gt;
    &lt;h2 id=&#34;goal--guardrail-metrics&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Goal + Guardrail metrics&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;When a measure becomes a target, it ceases to be a good measure&amp;rdquo; - Goodhart&amp;rsquo;s Law&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Goal metrics serve as a target, the specific thing you want to change or improve. While I largely agree with Goodhart&amp;rsquo;s Law, I also believe that a sufficiently comprehensive &lt;strong&gt;set&lt;/strong&gt; of metrics will help mitigate the downside (or at least minimize it) thus making it still &lt;em&gt;relatively&lt;/em&gt; &amp;ldquo;a good measure&amp;rdquo;. This is where guardrail metrics come in to ensure we aren&amp;rsquo;t just &amp;ldquo;sacrificing x to boost y&amp;rdquo; especially in an unsustainable manner.&lt;/p&gt;
&lt;p&gt;As an example, if you want to increase usage on your app, you can provide generous discounts (or even pay users to do so) but that&amp;rsquo;s obviously very unsustainable so you should set a clear budget on how much you can spend on these promotions and / or cost per new active user acquired you allow for such a project to ship. Here your goal metric would be &amp;ldquo;new active users acquired&amp;rdquo; while your guardrail would be cost (lost revenue). They need to both look good from the experiment before you decide on shipping the product change.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#size-and-duration&#34;&gt;
    &lt;h2 id=&#34;size-and-duration&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Size and Duration&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Generally, size and duration are the 2 main levers to help get your experiment sufficient exposure that allows you to make (statistically) informed decisions. If you don&amp;rsquo;t have a large user base then many of your tests would need a large test group with long test duration to show any statistically significant movement. This unfortunately means that your new feature release cycle will be much slower as your experiments will be queued. In this scenario, you will need to smartly pair up different features that complement one another into the same experiment.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#holdout&#34;&gt;
    &lt;h2 id=&#34;holdout&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Holdout&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;In order to accurately measure your team&amp;rsquo;s impact over a certain period of time, you can create a team-wide holdout that keeps a small group of users without the new enhancements. From here, you can see the value of all your team&amp;rsquo;s projects over that period of time (quarter / half) collectively. When you run experiments, you are only testing the impact of specific changes while here you are testing them collectively. Some changes conflict with each other while others complement each other. Ideally you catch them early and run variations of different combinations but you don&amp;rsquo;t always catch them all so having a team wide holdout helps with tracking that.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#novelty-effect&#34;&gt;
    &lt;h2 id=&#34;novelty-effect&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Novelty effect&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When a new restaurant first opens in a popular area, it will be filled with people lining up to try it out. These people will then attract more people who also decide to check out the restaurant out of curiosity. However, the crowd will slowly wind down over time as it loses its &lt;em&gt;novelty&lt;/em&gt; status. If it&amp;rsquo;s any good, it would still remain popular but unlikely to be as popular as it first opened. This is called novelty effect. When you add a new button to the screen, the user&amp;rsquo;s likelihood to click on it increases due to their curiosity on what it does. You don&amp;rsquo;t want to measure that. Instead, you want to measure the actual traffic (and effect) of it &lt;em&gt;after&lt;/em&gt; the novelty wears down. Just like how a good restaurant will still be popular long after its initial launch, you want your feature to be valuable / impactful even long after it&amp;rsquo;s been launched and users clearly understand what it does.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#security&#34;&gt;
    &lt;h2 id=&#34;security&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Security&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When making a security related change (fix or enhancement), a lot of these concepts become a bit trickier especially for critical issues. Mainly because bad actors can figure out that you are doing an A/B test based on their account ID / device ID / IP address and do targeted exploits on the unprotected test group. This makes it rather a futile effort to understand whatever topline metric movement you want to study of this change on regular benign users but instead, becomes more like opening a new window for exploits.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Running good A/B tests isn&amp;rsquo;t just about having the right tools—it&amp;rsquo;s about building the discipline to ask the right questions and measure what actually matters. The fundamentals covered here will prevent most common pitfalls that lead to misleading results or shipping changes that hurt your product. &lt;strong&gt;A/B testing is a means to an end.&lt;/strong&gt; The best growth teams quickly kill bad ideas and double down on what&amp;rsquo;s working. Start simple, get comfortable with the process, and don&amp;rsquo;t let perfect be the enemy of good—a simple, well-run test is infinitely better than a complex experiment that sits in planning forever.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
