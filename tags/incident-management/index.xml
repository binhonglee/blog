<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Incident Management on BinHong Lee&#39;s Blog</title>
    <link>https://binhong.me/blog/tags/incident-management/</link>
    <description>Recent content in Incident Management on BinHong Lee&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>binhong@binhong.me (BinHong Lee)</managingEditor>
    <webMaster>binhong@binhong.me (BinHong Lee)</webMaster>
    <lastBuildDate>Fri, 01 Aug 2025 00:00:00 -0800</lastBuildDate><atom:link href="https://binhong.me/blog/tags/incident-management/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Major Incident Runbook</title>
      <link>https://binhong.me/blog/2025-07-25-major-incident-runbook/</link>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-07-25-major-incident-runbook/</guid>
      <description>&lt;p&gt;I wrote a similar version of this internally at Meta a few years ago for my org after finding myself in the middle of a few SEV1s in a row &amp;ndash; and being consulted / asked for support in other similar situations. I thought this might be something useful to share (as a public version) as well. This won&amp;rsquo;t be perfectly fitting for all use cases, but having a runbook works as an anchor in the midst of chaos, helping to get you unstuck from &amp;ldquo;what&amp;rsquo;s next?&amp;rdquo;. Admittedly, this is an incomplete runbook that serves more as a template for your team or company to complete with more specific tooling guides (using &lt;em&gt;which&lt;/em&gt; tool to achieve &lt;em&gt;what&lt;/em&gt;, etc.).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        &lt;div class=&#34;newsletter&#34;&gt;
          &lt;h2&gt;Subscribe to the newsletter today!&lt;/h2&gt;
          &lt;form action=&#34;https://binhong.me/newsletter/subscribe&#34; method=&#34;post&#34;&gt;
              &lt;input type=&#34;email&#34; name=&#34;email&#34; placeholder=&#34;email@domain.com&#34;/&gt;
              &lt;noscript&gt;
                &lt;br /&gt;
                &lt;label&gt;You will need to manually get this secret from &lt;a href=&#34;https://binhong.me/newsletter/secret&#34;&gt;here&lt;/a&gt;&lt;/label&gt;
                &lt;input name=&#34;secret&#34; placeholder=&#34;secret&#34;/&gt;
                &lt;br /&gt;
              &lt;/noscript&gt;
              &lt;input type=&#34;hidden&#34; name=&#34;secret&#34; id=&#34;secret&#34; value=&#34;123potato123&#34;/&gt;
              &lt;button type=&#34;submit&#34; name=&#34;submit&#34; class=&#34;subscribe&#34;&gt;Subscribe&lt;/button&gt;
          &lt;/form&gt;
          You can also checkout previous issues &lt;a href=&#34;https://binhong.me/newsletter&#34; target=&#34;_blank&#34; style=&#34;text-decoration: underline;&#34;&gt;here&lt;/a&gt; before deciding.
        &lt;/div&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#stop-the-bleed&#34;&gt;
    &lt;h2 id=&#34;stop-the-bleed&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Stop the Bleed!&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;The highest priority is to stop the bleeding immediately. &lt;em&gt;Flip a killswitch, roll back changes, apply server-side fixes, apply client-side fixes&lt;/em&gt; - &lt;strong&gt;in that order&lt;/strong&gt;. A killswitch generally propagates faster, thus is preferred over code changes, but even then, prioritize rolling back changes instead of forward fixing. Forward fixing adds more unknown factors into the mix (because that&amp;rsquo;s more new code which could now cause new / different problems); rolling back, on the other hand, is more predictable. Server-side fixes over client-side fixes should be pretty obvious since you can guarantee the server version (as the service provider), but you can&amp;rsquo;t always force a client to update (native apps), or there could be some caching involved (web).&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#multiple-workstreams&#34;&gt;
    &lt;h2 id=&#34;multiple-workstreams&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Multiple Workstreams&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;During a major incident, there are generally multiple things that can or need to happen in parallel. Break them down into clear workstreams and delegate a domain expert to run each of them. If more things are discovered down the line (or the situation changes), switch up the workstream and get different people (domain experts) involved to run different things. Since you&amp;rsquo;re dealing with an incident (which are usually time sensitive), &lt;strong&gt;never hesitate to call people&lt;/strong&gt;. This needs to be said a lot because people constantly hesitate about false positives or getting on others&amp;rsquo; bad side for inaccurately calling them up. But you won&amp;rsquo;t know what you don&amp;rsquo;t know without getting the domain expert to show up and verify what you&amp;rsquo;re seeing.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#chat-management&#34;&gt;
    &lt;h2 id=&#34;chat-management&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Chat Management&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;Alongside having separate workstreams, you should also set up separate chat threads for each workstream to keep the &lt;em&gt;main chat&lt;/em&gt; low on noise. That said, make sure to announce the establishment and / or major milestones of each new workstream clearly within the main chat so everyone relevant is correctly included. You might be in a lot of different chats and things will be chaotic, so you will need to take extra care in ensuring that the right people are in the right chat to allow them to get their different tasks going. Everyone should still be in the main chat, but most discussion should happen in the workstream chat.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#sync-meetings&#34;&gt;
    &lt;h2 id=&#34;sync-meetings&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Sync Meetings&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;There are generally 2 types of sync meetings. First is the &amp;ldquo;everything just happened so we started a meeting to info dump and get everyone up to speed.&amp;rdquo; This meeting will always be chaotic with a lot of people joining to figure out what&amp;rsquo;s happening and what to do next. (There might also be people who only joined out of curiosity, but as long as they aren&amp;rsquo;t interrupting, they are the least of your problems - unless Zoom is not scaling lol.) Set out a clear goal, a set of tasks, and an owner for each of those tasks. Finally, set a follow-up date and time (usually in a few hours) for everyone to regroup with new findings and progress to determine next steps.&lt;/p&gt;
&lt;p&gt;The second type is the follow-up and / or recurring meeting. This will feel more like your regular team standup meeting except with a lot more urgency. It&amp;rsquo;s important to note that if there&amp;rsquo;s any major breakthrough, people shouldn&amp;rsquo;t wait until the next planned meeting time to report (because, well, this is an outage lol) but should instead share it with everyone immediately. This goes back to the previous point about chat management where you (as the incident manager) will need to monitor each workstream chat to catch something like this. After resolving the core issue, there might still be &lt;em&gt;important&lt;/em&gt; cleanup work needed to be handled with a similar level of urgency to prevent the same outage from happening again in a very short time. In these long-running incident cleanups, you might opt to have daily (or even twice-a-day) sync meetings until the cleanup is complete.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;As a whole, you will largely be more like a PM / TPM instead of a regular engineer when handling a lot of this &lt;em&gt;alignment&lt;/em&gt; work. If you think someone else is better suited to handle this (another more senior engineer or your manager, etc.), ask for help while you focus on the thing that you do best (likely as the subject matter expert on investigation or remediation). Remember that incident management is a skill that improves with practice - don&amp;rsquo;t expect to be perfect on your first few incidents, even experienced engineers can feel overwhelmed when everything is on fire. Don&amp;rsquo;t forget to acknowledge the team&amp;rsquo;s efforts since major incidents are stressful for everyone involved, and use this runbook as a starting point while adapting it based on your team&amp;rsquo;s specific needs and continuously improving your processes with each incident.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>No Blame SEV (Incident) Culture</title>
      <link>https://binhong.me/blog/2025-05-30-no-blame-sev-culture/</link>
      <pubDate>Fri, 30 May 2025 00:00:00 -0800</pubDate>
      <author>binhong@binhong.me (BinHong Lee)</author>
      <guid>https://binhong.me/blog/2025-05-30-no-blame-sev-culture/</guid>
      <description>&lt;p&gt;Every time there&amp;rsquo;s a major outage at Meta, the first question I get from friends and family is usually &lt;em&gt;&amp;ldquo;did they fire the person who caused it?&amp;rdquo;&lt;/em&gt; which is where I have to explain this concept of &lt;strong&gt;No Blame SEV Culture&lt;/strong&gt;. Especially for an outage so big that a significant number of users are affected, the &lt;em&gt;individual&lt;/em&gt; causing it likely does not have ill intent and there are likely multiple different processes and systems that failed along the way to get us here in the first place.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is part of a series &lt;a href=&#34;https://binhong.me/blog/2025-05-04-the-opinionated-engineer/&#34;&gt;(The Opinionated Engineer)&lt;/a&gt; where I share my strong opinions on engineering practices.&lt;/em&gt;&lt;/p&gt;
        &lt;div class=&#34;newsletter&#34;&gt;
          &lt;h2&gt;Subscribe to the newsletter today!&lt;/h2&gt;
          &lt;form action=&#34;https://binhong.me/newsletter/subscribe&#34; method=&#34;post&#34;&gt;
              &lt;input type=&#34;email&#34; name=&#34;email&#34; placeholder=&#34;email@domain.com&#34;/&gt;
              &lt;noscript&gt;
                &lt;br /&gt;
                &lt;label&gt;You will need to manually get this secret from &lt;a href=&#34;https://binhong.me/newsletter/secret&#34;&gt;here&lt;/a&gt;&lt;/label&gt;
                &lt;input name=&#34;secret&#34; placeholder=&#34;secret&#34;/&gt;
                &lt;br /&gt;
              &lt;/noscript&gt;
              &lt;input type=&#34;hidden&#34; name=&#34;secret&#34; id=&#34;secret&#34; value=&#34;123potato123&#34;/&gt;
              &lt;button type=&#34;submit&#34; name=&#34;submit&#34; class=&#34;subscribe&#34;&gt;Subscribe&lt;/button&gt;
          &lt;/form&gt;
          You can also checkout previous issues &lt;a href=&#34;https://binhong.me/newsletter&#34; target=&#34;_blank&#34; style=&#34;text-decoration: underline;&#34;&gt;here&lt;/a&gt; before deciding.
        &lt;/div&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#process-over-people&#34;&gt;
    &lt;h2 id=&#34;process-over-people&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Process over People&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;When something goes wrong (especially something &lt;em&gt;really catastrophic&lt;/em&gt;), it&amp;rsquo;s usually a combination of both process and people problems. The difference here is that process is more deterministic compared to people. People have off-days, get tired, make mistakes etc. so it&amp;rsquo;s important to have a process (or automated systems) in place to prevent that. This can mean anything from adding more test coverage, lint rules against bad code patterns, and / or more alerts. It is however important to note that they need to &lt;strong&gt;maintain a certain level of quality bar&lt;/strong&gt;. As mentioned in &lt;a href=&#34;https://binhong.me/blog/2025-05-04-push-fearlessly-with-automated-testing/#broken-tests&#34;&gt;the previous article&lt;/a&gt;, flaky / broken tests are tech debt, same goes for noisy lint rules and alerts. Too many noisy lint rules and alerts would lead to engineers disregarding them or adopting a &amp;ldquo;wait-and-see&amp;rdquo; mentality which is not ideal in preventing future outages.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://binhong.me/blog/img/sev_review_trifecta.jpeg&#34; target=&#34;_blank&#34;&gt;
    &lt;img src=&#34;https://binhong.me/blog/2025-05-30-no-blame-sev-culture//blog/img/sev_review_trifecta.jpeg&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#expensive-lesson&#34;&gt;
    &lt;h2 id=&#34;expensive-lesson&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Expensive Lesson&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;One of the more interesting quotes I&amp;rsquo;ve read repeatedly (both within and outside of Meta) about people who caused outages is that they just learned an expensive lesson through that specific outage. Firing them (or letting them go) would mean that your company just paid that expensive price of such a lesson for an employee without actually benefiting from it. This employee will then bring this lesson with them to their next employer who would then benefit from such experience.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#fear&#34;&gt;
    &lt;h2 id=&#34;fear&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Fear&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;One of the more significant downsides of &lt;em&gt;blame&lt;/em&gt;, is that you now instill fear in making any sort of production changes (even calculated ones). Instead, it&amp;rsquo;s important to keep in mind that as your product / infra grows, so should your process. Having strong fear in taking any responsibility for even attempting to improve or make fundamental changes breeds complacency. This can be fine in certain organizations and products (like government software, health tech etc.) where there&amp;rsquo;s almost no tolerance for any sort of outages. That said, this is where you should have good chaos engineering and fail-safe practices to ensure the resiliency of your system.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#no-blame--no-responsibility&#34;&gt;
    &lt;h2 id=&#34;no-blame--no-responsibility&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;No Blame â‰  No Responsibility&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;This is a bit of an exception or outlier effect (and likely the most controversial part of this whole piece). Usually when you cause a really major outage (or multiple for that matter), it&amp;rsquo;s really not your fault (or shouldn&amp;rsquo;t be). But sometimes, smaller outages are understandably less &amp;ldquo;well protected&amp;rdquo; because we expect people to still &lt;em&gt;care&lt;/em&gt; about the things they work on. If you &lt;strong&gt;continuously&lt;/strong&gt; cause outages due to &lt;strong&gt;recklessness&lt;/strong&gt; (&amp;ldquo;lack of &lt;em&gt;care&lt;/em&gt;&amp;rdquo;) especially within a short period of time, you should still be held accountable for it. It&amp;rsquo;s especially common when someone chases the topline metrics movement against a tight timeline (end of a performance review cycle). This does not mean that people should be finger-pointing during the incident review, as before, that should be used to focus on what could&amp;rsquo;ve been better instead. However, it should be brought up separately as part of the performance conversation. It&amp;rsquo;s important to note that this is a scenario where a &lt;em&gt;quantitative change leads to a qualitative change&lt;/em&gt; since an increase in quantity (of incidents) leads to a change in narrative thus should not be used to penalize those who&amp;rsquo;ve only caused one (or maybe two) incidents in a given period of time.&lt;/p&gt;
&lt;p&gt;Aside from that, if there isn&amp;rsquo;t any runbook or recovery plan prepared ahead of time (especially for predictable issues - &lt;em&gt;*subjective*&lt;/em&gt;), it demonstrates a lack of good planning and foresight into the feature. This is - frankly - a lack of competence and the project owner should take responsibility for the rather &lt;em&gt;incomplete&lt;/em&gt; launch. However, the reality is that many individuals would launch buggy projects, claim credit for all the good it brings, while oncalls (spread across the team) pay for the lack of implementation quality. This is especially true when they immediately switch teams after project launches and no longer have to maintain or deal with the aftermath of their uninspiring launch.&lt;/p&gt;
&lt;a class=&#34;anchor&#34; href=&#34;#wrap-up&#34;&gt;
    &lt;h2 id=&#34;wrap-up&#34;&gt;
        &lt;span class=&#34;text&#34;&gt;Wrap up&lt;/span&gt;
        &lt;span class=&#34;tag&#34;&gt;#&lt;/span&gt;
    &lt;/h2&gt;
&lt;/a&gt;
&lt;p&gt;No blame culture means that you aren&amp;rsquo;t fully responsible just because you accidentally touched the house of cards causing it to collapse. Instead, we need better protection around it - like building a fence around it, using LEGO blocks instead of cards, etc - to make sure it doesn&amp;rsquo;t break down easily again after someone accidentally touches it, or just prevent people from accidentally touching it altogether. This means we hold those who are responsible for ensuring the protection accountable instead of those who inevitably discovered the problem. It&amp;rsquo;s like how we don&amp;rsquo;t blame white hat hackers for discovering an exploit; we pay them bounties as a way to thank them for discovering them. We should thank those who found holes in our system&amp;rsquo;s reliability instead of penalizing them for finding it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
